{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install praat-textgrids\n!pip install jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-25T23:26:33.470269Z","iopub.execute_input":"2023-07-25T23:26:33.470630Z","iopub.status.idle":"2023-07-25T23:27:00.189500Z","shell.execute_reply.started":"2023-07-25T23:26:33.470599Z","shell.execute_reply":"2023-07-25T23:27:00.188343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom absl import flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('normalizers_file', '/kaggle/input/normalizers/normalizers.pkl', 'file with pickled feature normalizers')\n\nflags.DEFINE_list('remove_channels', [], 'channels to remove')\nflags.DEFINE_list('silent_data_directories', ['/kaggle/input/emgdata/emg_data/silent_parallel_data'], 'silent data locations')\nflags.DEFINE_list('voiced_data_directories', ['/kaggle/input/emgdata/emg_data/voiced_parallel_data','/kaggle/input/emgdata/emg_data/nonparallel_data'], 'voiced data locations')\nflags.DEFINE_string('testset_file', '/kaggle/input/testesetlarge/testset_largedev.json', 'file with testset indices')\nflags.DEFINE_string('text_align_directory', '/kaggle/input/textaligns/text_alignments', 'directory with alignment files')\n\nflags.DEFINE_boolean('debug', False, 'debug')\nflags.DEFINE_string('output_directory', '/kaggle/working/outputs', 'where to save models and outputs')\nflags.DEFINE_integer('batch_size', 32, 'training batch size')\nflags.DEFINE_float('learning_rate', 3e-4, 'learning rate')\nflags.DEFINE_integer('learning_rate_warmup', 1000, 'steps of linear warmup')\nflags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')\nflags.DEFINE_string('start_training_from', None, 'start training from this model')\nflags.DEFINE_float('l2', 0, 'weight decay')\nflags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')\n\nFLAGS(sys.argv[1:])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:27:00.192109Z","iopub.execute_input":"2023-07-25T23:27:00.193427Z","iopub.status.idle":"2023-07-25T23:27:00.226039Z","shell.execute_reply.started":"2023-07-25T23:27:00.193396Z","shell.execute_reply":"2023-07-25T23:27:00.224991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile data_utils.py\n\nimport string\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom textgrids import TextGrid\nimport jiwer\nfrom unidecode import unidecode\n\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom absl import flags\nFLAGS = flags.FLAGS\n\nphoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']\n\ndef normalize_volume(audio): #recebe um sinal de áudio e realiza uma normalização de volume nele.\n    rms = librosa.feature.rms(audio)  #calcula o valor RMS (root mean square) do sinal de áudio \n    max_rms = rms.max() + 0.01\n    target_rms = 0.2\n    audio = audio * (target_rms/max_rms) #normaliza o sinal de áudio multiplicando-o por um fator que ajusta o RMS para um valor de destino (0.2)\n    max_val = np.abs(audio).max()\n    if max_val > 1.0: # this shouldn't happen too often with the target_rms of 0.2\n        audio = audio / max_val  #Se o valor máximo absoluto do sinal de áudio for maior que 1.0, o sinal é dividido pelo valor máximo para evitar a saturação.\n    return audio\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):  #realiza uma compressão de faixa dinâmica em um tensor\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\ndef spectral_normalize_torch(magnitudes):  #realiza a normalização espectral\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\nmel_basis = {}\nhann_window = {}\n\ndef mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):  # calcula o espectrograma mel de um sinal de áudio\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    if fmax not in mel_basis: #verificam se já foi calculada a base de mel correspondente à frequência máxima fmax. Se não tiver sido calculada, a função librosa.filters.mel \n                                #é usada para calcular a base de mel com os parâmetros fornecidos\n        mel = librosa.filters.mel(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n\n        #print('Mel ', mel)\n        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n        #print('Mel_basis ', mel_basis)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect') #preenche o sinal de áudio y com reflexão antes do cálculo do espectrograma. \n                                                                                #O sinal é expandido com uma dimensão extra e é aplicado um preenchimento refletivo em ambas as extremidades.\n    y = y.squeeze(1) #a dimensão extra é removida.\n    #print('y ', y)\n    #spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)], #calcula a transformada de Fourier de curto tempo (STFT) do sinal de áudio y\n                      #center=center, pad_mode='reflect', normalized=False, onesided=True)  #O espectrograma é calculado apenas para a metade positiva das frequências (onesided=True).\n    #print ('Resutado do stft ', spec)\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n                  center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n    #print ('Resutado do stft ', spec)\n    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9)) #Essa linha calcula o módulo (magnitude) do espectrograma. O espectrograma é elevado ao quadrado, somado ao valor de 1e-9 para evitar divisão por zero\n    # Imprimir as dimensões de mel_basis\n    #print ('Resutado do sqrt ', spec)\n    #print(len(mel_basis))\n    #for key, value in mel_basis.items():\n        #print(f\"Dimensões de {key}: {value.shape}\")\n    #print('Dimensões de spec ', spec.shape)\n    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n\n    \n    #spec = mel_basis[str(fmax)+'_'+str(y.device)] @ spec\n\n    #print ('Resutado da multiplicação ', spec)\n    #print('Dimensões de spec ', spec.shape)\n    spec = spectral_normalize_torch(spec) #O resultado é normalizado espectralmente\n\n    return spec\n\ndef load_audio(filename, start=None, end=None, max_frames=None, renormalize_volume=False): #carrega um arquivo de áudio, realiza pré-processamento nele e retorna o espectrograma mel correspondente\n    audio, r = sf.read(filename)\n\n    if len(audio.shape) > 1:\n        audio = audio[:,0] # verifica se o sinal de áudio possui mais de uma dimensão, o que indicaria que é um áudio estéreo. Nesse caso, seleciona apenas o primeiro canal de áudio.\n    if start is not None or end is not None:\n        audio = audio[start:end] #permitem selecionar uma parte específica do sinal de áudio\n\n    if renormalize_volume:\n        audio = normalize_volume(audio)\n    if r == 16000:\n        audio = librosa.resample(audio, orig_sr=16000, target_sr=22050)  #verificam a taxa de amostragem r do áudio. Se for igual a 16000, o áudio é ressampleado para a taxa de amostragem de 22050 Hz usando a função librosa.resample. \n    else:\n        assert r == 22050\n    audio = np.clip(audio, -1, 1) # realiza um ajuste no intervalo de valores do sinal de áudio. Por vezes, o ressampleamento pode fazer com que alguns valores ultrapassem o intervalo [-1, 1], \n                                    #então essa linha garante que todos os valores estejam dentro desse intervalo.\n    pytorch_mspec = mel_spectrogram(torch.tensor(audio, dtype=torch.float32).unsqueeze(0), 1024, 80, 22050, 256, 1024, 0, 8000, center=False)  #calculam o espectrograma mel do sinal de áudio\n    mspec = pytorch_mspec.squeeze(0).T.numpy() #O resultado é convertido em uma matriz numpy e atribuído a mspec.\n    if max_frames is not None and mspec.shape[0] > max_frames:  #verifica se o parâmetro max_frames foi fornecido e se o número de quadros (linhas) do espectrograma excede max_frames\n        mspec = mspec[:max_frames,:] #o espectrograma é recortado para ter no máximo max_frames quadros.\n    return mspec\n\ndef double_average(x):  #suaviza um sinal x aplicando uma média móvel dupla, ou seja, realizando duas convoluções consecutivas com um filtro médio. Isso ajuda a reduzir o ruído e suavizar o sinal.\n    assert len(x.shape) == 1  # verificam se o sinal x tem apenas uma dimensão. Caso contrário, é lançado um erro.\n    f = np.ones(9)/9.0  #cria uma matriz de tamanho 9 preenchida com o valor 1.0 e, em seguida, divide todos os elementos por 9.0. Essa matriz f será usada como um filtro médio.\n    v = np.convolve(x, f, mode='same')  #aplica a convolução entre o sinal x e o filtro f. A opção mode='same' garante que o tamanho da saída seja o mesmo que o tamanho de x\n    w = np.convolve(v, f, mode='same')  #aplica a convolução entre o sinal v e o filtro f. A opção mode='same' garante que o tamanho da saída seja o mesmo que o tamanho de x\n    return w\n\ndef get_emg_features(emg_data, debug=False): #calcula recursos relacionados ao sinal de eletromiografia (EMG) a partir dos dados de EMG fornecido. Esses recursos incluem médias de janelas, valores RMS, \n                                    #taxa de cruzamento por zero e espectrograma de curto prazo. Esses recursos podem ser usados posteriormente para análise e processamento adicional dos sinais de EMG.\n    xs = emg_data - emg_data.mean(axis=0, keepdims=True)  #calculam xs, que é o sinal de EMG centrado em torno da média. \n                                                            #É subtraída a média de cada coluna dos dados de EMG (emg_data) usando a função mean ao longo do eixo 0.\n    frame_features = []\n    for i in range(emg_data.shape[1]):\n        x = xs[:,i]  # x: coluna atual de xs\n        w = double_average(x) # w: sinal resultante da aplicação da função double_average em x, ou seja, o sinal suavizado\n        p = x - w # p: sinal resultante da subtração de w de x, representando as partes pulsativas do sinal\n        r = np.abs(p) #r: sinal resultante do valor absoluto de p, representando a magnitude das partes pulsativas do sinal\n\n        w_h = librosa.util.frame(w, frame_length=16, hop_length=6).mean(axis=0) # w_h: média das janelas de 16 amostras de w com um deslocamento de 6 amostras\n        p_w = librosa.feature.rms(y=w, frame_length=16, hop_length=6, center=False)  #p_w: valor RMS (Root Mean Square) das janelas de 16 amostras de w com um deslocamento de 6 amostras\n        p_w = np.squeeze(p_w, 0)\n        p_r = librosa.feature.rms(y=r, frame_length=16, hop_length=6, center=False)  #p_r: valor RMS das janelas de 16 amostras de r com um deslocamento de 6 amostras\n        p_r = np.squeeze(p_r, 0)\n        z_p = librosa.feature.zero_crossing_rate(p, frame_length=16, hop_length=6, center=False) #z_p: taxa de cruzamento por zero das janelas de 16 amostras de p com um deslocamento de 6 amostras\n        z_p = np.squeeze(z_p, 0)\n        r_h = librosa.util.frame(r, frame_length=16, hop_length=6).mean(axis=0) #r_h: média das janelas de 16 amostras de r com um deslocamento de 6 amostras\n\n        s = abs(librosa.stft(np.ascontiguousarray(x), n_fft=16, hop_length=6, center=False))  #calcula o espectrograma de curto prazo do sinal x usando a Transformada de Fourier de Curto Prazo (STFT)\n        # s has feature dimension first and time second\n\n        if debug:\n            plt.subplot(7,1,1)\n            plt.plot(x)\n            plt.subplot(7,1,2)\n            plt.plot(w_h)\n            plt.subplot(7,1,3)\n            plt.plot(p_w)\n            plt.subplot(7,1,4)\n            plt.plot(p_r)\n            plt.subplot(7,1,5)\n            plt.plot(z_p)\n            plt.subplot(7,1,6)\n            plt.plot(r_h)\n\n            plt.subplot(7,1,7)\n            plt.imshow(s, origin='lower', aspect='auto', interpolation='nearest')\n\n            plt.show()\n\n        frame_features.append(np.stack([w_h, p_w, p_r, z_p, r_h], axis=1))  #empilham os recursos calculados para cada coluna em uma lista. Os recursossão empilhados verticalmente, resultando em uma matriz 2D.\n        frame_features.append(s.T) #o espectrograma s é transposto e adicionado à lista frame_features.\n\n    frame_features = np.concatenate(frame_features, axis=1) #concatena todos os elementos da lista frame_features ao longo do eixo 1, resultando em uma matriz unidimensional final\n    return frame_features.astype(np.float32)\n\nclass FeatureNormalizer(object):  #implementa um normalizador de recursos.Ela fornece métodos para normalizar uma amostra de recurso e desfazer a normalização, \n                #aplicando as médias e os desvios padrão calculados. Esse normalizador pode ser útil para preparar os dados antes de usá-los em um modelo de aprendizado de máquina.\n    def __init__(self, feature_samples, share_scale=False):#é o construtor da classe. Ele recebe uma lista de amostras de recursos (feature_samples), que são matrizes 2D com dimensões (tempo, recurso). \n        \"\"\" features_samples should be list of 2d matrices with dimension (time, feature) \"\"\"\n        feature_samples = np.concatenate(feature_samples, axis=0)  #as amostras de recursos são concatenadas ao longo do eixo 0 para formar uma única matriz\n        self.feature_means = feature_samples.mean(axis=0, keepdims=True)  #as médias dos recursos são calculadas ao longo do eixo 0 e armazenadas em self.feature_means\n        if share_scale:\n            self.feature_stddevs = feature_samples.std()  #Se share_scale for True, o desvio padrão de todos os recursos é calculado e armazenado em self.feature_stddevs\n        else:\n            self.feature_stddevs = feature_samples.std(axis=0, keepdims=True) #Caso contrário, os desvios padrão de cada recurso são calculados separadamente e armazenados em self.feature_stddevs.\n\n    def normalize(self, sample): #recebe uma amostra de recurso (sample) e normaliza essa amostra subtraindo as médias dos recursos (self.feature_means) e dividindo pelo desvio padrão dos recursos\n        sample -= self.feature_means\n        sample /= self.feature_stddevs\n        return sample\n\n    def inverse(self, sample): #ecebe uma amostra de recurso normalizada e realiza a operação inversa da normalização. Primeiro, a amostra é multiplicada pelo desvio padrão dos recursos. \n                                #Em seguida, a média dos recursos (self.feature_means) é adicionada de volta à amostra. \n        sample = sample * self.feature_stddevs\n        sample = sample + self.feature_means\n        return sample\n\ndef combine_fixed_length(tensor_list, length): #combina uma lista de tensores em um único tensor de comprimento fixo. \n                                #Ele garante que os dados sejam combinados em um tensor de tamanho fixo, preenchendo com zeros, se necessário.\n    total_length = sum(t.size(0) for t in tensor_list) #calcula o comprimento total somando o tamanho (dimensão 0) de cada tensor na lista.\n    if total_length % length != 0:#verifica se o comprimento total não é divisível pelo comprimento desejado\n        pad_length = length - (total_length % length)  #Se não for, calcula o comprimento de preenchimento necessário para tornar o total divisível pelo comprimento desejado\n        tensor_list = list(tensor_list) # copy\n        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device)) #um tensor preenchido com zeros é criado com o comprimento de \n                                                                #preenchimento necessário e as mesmas dimensões do primeiro tensor na lista. Esse tensor de preenchimento é anexado à lista de tensores.\n        total_length += pad_length \n    tensor = torch.cat(tensor_list, 0)  #os tensores na lista (incluindo o tensor de preenchimento, se adicionado) são concatenados ao longo da dimensão 0 para formar um único tensor.\n    n = total_length // length #o número de segmentos de comprimento fixo que podem ser extraídos do tensor é calculado dividindo o comprimento total pelo comprimento desejado.\n    return tensor.view(n, length, *tensor.size()[1:])  #o tensor é redimensionado para ter as dimensões (n, length, ...), onde n é o número de segmentos e ... representa as dimensões restantes dos tensores originais.\n\ndef decollate_tensor(tensor, lengths):  #desagrega um tensor em uma lista de tensores com comprimentos diferentes. Essa função é útil quando você deseja separar um tensor em segmentos de comprimentos diferentes, \n                                    #conforme especificado por uma lista de comprimentos. Pode ser usado, por exemplo, para processar lotes de dados em tamanhos diferentes após a etapa de inferência em um modelo.\n    b, s, d = tensor.size()  # obtem as dimensões do tensor original. b representa o tamanho do lote, s representa o comprimento dos segmentos e d representa a dimensão dos recursos.\n    tensor = tensor.view(b*s, d) # o tensor é redimensionado usando .view() para ter uma forma de (b * s, d). Isso combina o tamanho do lote com o comprimento dos segmentos.\n    results = []\n    idx = 0\n    for length in lengths: #Para cada comprimento, é verificado se a posição atual mais o comprimento está dentro dos limites do tensor. Se não estiver, um erro de assert é acionado.\n        assert idx + length <= b * s\n        results.append(tensor[idx:idx+length]) #o segmento correspondente é extraído do tensor, começando na posição idx e com o comprimento especificado. O segmento é adicionado à lista results.\n        idx += length\n    return results\n\ndef splice_audio(chunks, overlap): #combina várias partes de áudio sobrepostas em um único áudio.Essa função é útil para combinar partes de áudio sobrepostas, como segmentos de áudio em uma gravação \n                                        #contínua, onde a sobreposição ajuda a suavizar a transição entre as partes.\n    chunks = [c.copy() for c in chunks] # copy so we can modify in place\n\n    assert np.all([c.shape[0]>=overlap for c in chunks]) #é verificado se todas as partes de áudio têm um tamanho maior ou igual à sobreposição especificada. Isso é importante para garantir \n                                                            #que haja dados suficientes para aplicar a sobreposição corretamente.\n\n    result_len = sum(c.shape[0] for c in chunks) - overlap*(len(chunks)-1) #soma os tamanhos de todas as partes de áudio e subtraindo o tamanho da sobreposição entre as partes para o comprimento total\n    result = np.zeros(result_len, dtype=chunks[0].dtype)  #Um array de zeros chamado result é inicializado com o comprimento total calculado e o tipo de dados da primeira parte de áudio.\n\n    ramp_up = np.linspace(0,1,overlap) #Duas rampas, ramp_up e ramp_down, são criadas usando np.linspace() para representar as funções de aumento e diminuição gradual da amplitude durante a sobreposição.\n    ramp_down = np.linspace(1,0,overlap)\n\n    i = 0\n    for chunk in chunks:\n        l = chunk.shape[0]  #Em um loop for, cada parte de áudio é processada individualmente. Para cada parte de áudio, seu comprimento l é obtido.\n\n        # note: this will also fade the beginning and end of the result\n        chunk[:overlap] *= ramp_up  #As partes de áudio são multiplicadas pelos valores correspondentes nas rampas ramp_up e ramp_down. Isso aplica a sobreposição gradual no início e no final de cada parte de áudio.\n        chunk[-overlap:] *= ramp_down\n\n        result[i:i+l] += chunk #A parte de áudio processada é adicionada ao resultado final a partir da posição i. A variável i é atualizada para a próxima posição correta no resultado, levando em \n                                    #consideração o tamanho da parte de áudio e a sobreposição.\n        i += l-overlap\n\n    return result\n\ndef print_confusion(confusion_mat, n=10): #imprime informações sobre as confusões mais comuns em uma matriz de confusão. Essa função é útil para analisar e visualizar as confusões mais comuns em uma matriz de \n                            #confusão, fornecendo insights sobre o desempenho do modelo de classificação em relação a classes específicas.\n    # axes are (pred, target)\n    target_counts = confusion_mat.sum(0) + 1e-4  #calcula o número de ocorrências de cada classe alvo na matriz de confusão. Isso é feito somando os valores de cada coluna da matriz e adicionando um \n                                    #pequeno valor (1e-4) para evitar divisão por zero.\n    aslist = []\n    for p1 in range(len(phoneme_inventory)): #a função itera sobre todas as combinações únicas de classes alvo (p1 e p2). As confusões são calculadas somando as ocorrências nas células \n                                    #correspondentes na matriz de confusão e dividindo pelo número total de ocorrências das duas classes alvo.\n        for p2 in range(p1):\n            if p1 != p2:\n                aslist.append(((confusion_mat[p1,p2]+confusion_mat[p2,p1])/(target_counts[p1]+target_counts[p2]), p1, p2)) #As confusões são armazenadas em uma lista aslist como uma tupla contendo a taxa de confusão,\n                                                                                                                #o índice p1 da classe alvo, e o índice p2 da classe alvo.\n    aslist.sort() #classifica em ordem crescente com base na taxa de confusão\n    aslist = aslist[-n:]  #é selecionado o top n das confusões mais comuns.\n    max_val = aslist[-1][0]  #O valor máximo e o valor mínimo de confusão são obtidos a partir da lista aslist.\n    min_val = aslist[0][0]\n    val_range = max_val - min_val\n    print('Common confusions (confusion, accuracy)') \n    for v, p1, p2 in aslist:\n        p1s = phoneme_inventory[p1]\n        p2s = phoneme_inventory[p2]\n        print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')\n        #^ imprime as informações sobre as confusões mais comuns. Ela exibe a classe alvo p1, a classe alvo p2, a taxa de confusão (multiplicada por 100 para exibição em porcentagem) e a precisão (também \n        # multiplicada por 100).\n\ndef read_phonemes(textgrid_fname, max_len=None): #lê os fonemas de um arquivo TextGrid e os converte em uma sequência de índices de fonemas, usada para treinar e avaliar modelos de processamento de linguagem.\n    tg = TextGrid(textgrid_fname)\n    phone_ids = np.zeros(int(tg['phones'][-1].xmax*86.133)+1, dtype=np.int64)  #cria um array phone_ids preenchido com valores -1. O tamanho do array é calculado com base no tempo máximo encontrado \n            #no arquivo TextGrid multiplicado por 86.133. O valor 86.133 é usado para converter o tempo do TextGrid para a taxa de amostragem de 22050 Hz, que é a taxa de amostragem mencionada anteriormente.\n    phone_ids[:] = -1  \n    phone_ids[-1] = phoneme_inventory.index('sil') #o último valor do array é definido como o índice do fonema 'sil' no inventário de fonemas. Isso garante que a lista seja longa o suficiente para cobrir \n                                                    #todo o comprimento da sequência original.\n    for interval in tg['phones']: #percorre cada intervalo de fonema no arquivo TextGrid e mapeia o fonema correspondente para o seu índice no inventário de fonemas.\n        phone = interval.text.lower() \n        if phone in ['', 'sp', 'spn']: # Se o fonema for uma string vazia, 'sp' ou 'spn', ele é substituído por 'sil'\n            phone = 'sil'\n        if phone[-1] in string.digits: #Se o fonema tiver um dígito no final, o dígito é removido.\n            phone = phone[:-1]\n        ph_id = phoneme_inventory.index(phone) #Os índices de fonemas são atribuídos aos intervalos de tempo correspondentes no array phone_ids.\n        phone_ids[int(interval.xmin*86.133):int(interval.xmax*86.133)] = ph_id\n    assert (phone_ids >= 0).all(), 'missing aligned phones' #verifica se todos os valores de phone_ids são não negativos (ou seja, todos os fonemas foram encontrados e mapeados corretamente). \n                                #Caso contrário, uma exceção é lançada indicando que há fonemas ausentes na sequência alinhada.\n\n    if max_len is not None:  #Se max_len for especificado, a sequência de fonemas é truncada para o comprimento máximo especificado\n        phone_ids = phone_ids[:max_len]\n        assert phone_ids.shape[0] == max_len #A função verifica se o comprimento da sequência truncada é igual a max_len.\n    return phone_ids\n\nclass TextTransform(object): #implementa transformações de texto úteis, como limpeza de texto, mapeamento de texto para sequências de números inteiros e mapeamento inverso de sequências de números inteiros para texto.\n    def __init__(self):\n        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])  #transformation é um objeto jiwer.Compose que encapsula uma série de transformações de texto, como remoção de pontuação \n                                #e conversão para minúsculas. Essas transformações são realizadas pelo pacote jiwer.\n        self.chars = string.ascii_lowercase+string.digits+' ' #chars é uma string que contém todos os caracteres permitidos no texto, incluindo letras minúsculas, dígitos e espaço em branco.\n\n    def clean_text(self, text): #Limpa o texto aplicando as transformações definidas.\n        text = unidecode(text)  #remove quaisquer caracteres acentuados ou diacríticos\n        text = self.transformation(text) #as transformações definidas em self.transformation são aplicadas\n        return text\n\n    def text_to_int(self, text): #Converte o texto em uma sequência de números inteiros\n        text = self.clean_text(text) #O texto é limpo usando o método clean_text\n        return [self.chars.index(c) for c in text] #cada caractere do texto limpo é mapeado para o seu índice correspondente em self.chars\n\n    def int_to_text(self, ints):  #Converte uma sequência de números inteiros em texto\n        return ''.join(self.chars[i] for i in ints)  #Cada número inteiro em ints é mapeado para o caractere correspondente em self.chars. ","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:27:00.227949Z","iopub.execute_input":"2023-07-25T23:27:00.228434Z","iopub.status.idle":"2023-07-25T23:27:00.254218Z","shell.execute_reply.started":"2023-07-25T23:27:00.228401Z","shell.execute_reply":"2023-07-25T23:27:00.253169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile read_emg.py\n\nimport re\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom collections import defaultdict\nimport scipy\nimport json\nimport copy\nimport sys\nimport pickle\nimport string\nimport logging\nfrom functools import lru_cache\nfrom copy import copy\n\n\n\nimport librosa\nimport soundfile as sf\n\nimport torch\n\nfrom data_utils import load_audio, get_emg_features, FeatureNormalizer, phoneme_inventory, read_phonemes, TextTransform\n\nfrom scipy import signal\n\nfrom absl import flags\nFLAGS = flags.FLAGS\n\ndef remove_drift(signal, fs):\n    b, a = scipy.signal.butter(3, 2, 'highpass', fs=fs)\n    return scipy.signal.filtfilt(b, a, signal)\n\ndef notch(signal, freq, sample_frequency):\n    b, a = scipy.signal.iirnotch(freq, 30, sample_frequency)\n    return scipy.signal.filtfilt(b, a, signal)\n\ndef notch_harmonics(signal, freq, sample_frequency):\n    for harmonic in range(1,8):\n        signal = notch(signal, freq*harmonic, sample_frequency)\n    return signal\n\ndef subsample(signal, new_freq, old_freq):\n    times = np.arange(len(signal))/old_freq\n    sample_times = np.arange(0, times[-1], 1/new_freq)\n    result = np.interp(sample_times, times, signal)\n    return result\n\ndef apply_to_all(function, signal_array, *args, **kwargs):\n    results = []\n    for i in range(signal_array.shape[1]):\n        results.append(function(signal_array[:,i], *args, **kwargs))\n    return np.stack(results, 1)\n\ndef load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):\n    index = int(index)\n    raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))\n    before = os.path.join(base_dir, f'{index-1}_emg.npy')\n    after = os.path.join(base_dir, f'{index+1}_emg.npy')\n    if os.path.exists(before):\n        raw_emg_before = np.load(before)\n    else:\n        raw_emg_before = np.zeros([0,raw_emg.shape[1]])\n    if os.path.exists(after):\n        raw_emg_after = np.load(after)\n    else:\n        raw_emg_after = np.zeros([0,raw_emg.shape[1]])\n\n    x = np.concatenate([raw_emg_before, raw_emg, raw_emg_after], 0)\n    x = apply_to_all(notch_harmonics, x, 60, 1000)\n    x = apply_to_all(remove_drift, x, 1000)\n    x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]\n    emg_orig = apply_to_all(subsample, x, 689.06, 1000)\n    x = apply_to_all(subsample, x, 516.79, 1000)\n    emg = x\n\n    for c in FLAGS.remove_channels:\n        emg[:,int(c)] = 0\n        emg_orig[:,int(c)] = 0\n\n    emg_features = get_emg_features(emg)\n\n    mfccs = load_audio(os.path.join(base_dir, f'{index}_audio_clean.flac'),\n            max_frames=min(emg_features.shape[0], 800 if limit_length else float('inf')))\n\n    if emg_features.shape[0] > mfccs.shape[0]:\n        emg_features = emg_features[:mfccs.shape[0],:]\n    assert emg_features.shape[0] == mfccs.shape[0]\n    emg = emg[6:6+6*emg_features.shape[0],:]\n    emg_orig = emg_orig[8:8+8*emg_features.shape[0],:]\n    assert emg.shape[0] == emg_features.shape[0]*6\n\n    with open(os.path.join(base_dir, f'{index}_info.json')) as f:\n        info = json.load(f)\n\n    sess = os.path.basename(base_dir)\n    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'\n    if os.path.exists(tg_fname):\n        phonemes = read_phonemes(tg_fname, mfccs.shape[0])\n    else:\n        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')\n\n    return mfccs, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)\n\nclass EMGDirectory(object):\n    def __init__(self, session_index, directory, silent, exclude_from_testset=False):\n        self.session_index = session_index\n        self.directory = directory\n        self.silent = silent\n        self.exclude_from_testset = exclude_from_testset\n\n    def __lt__(self, other):\n        return self.session_index < other.session_index\n\n    def __repr__(self):\n        return self.directory\n\nclass SizeAwareSampler(torch.utils.data.Sampler):\n    def __init__(self, emg_dataset, max_len):\n        self.dataset = emg_dataset\n        self.max_len = max_len\n\n    def __iter__(self):\n        indices = list(range(len(self.dataset)))\n        random.shuffle(indices)\n        batch = []\n        batch_length = 0\n        for idx in indices:\n            directory_info, file_idx = self.dataset.example_indices[idx]\n            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:\n                info = json.load(f)\n            if not np.any([l in string.ascii_letters for l in info['text']]):\n                continue\n            length = sum([emg_len for emg_len, _, _ in info['chunks']])\n            if length > self.max_len:\n                logging.warning(f'Warning: example {idx} cannot fit within desired batch length')\n            if length + batch_length > self.max_len:\n                yield batch\n                batch = []\n                batch_length = 0\n            batch.append(idx)\n            batch_length += length\n        # dropping last incomplete batch\n\nclass EMGDataset(torch.utils.data.Dataset):\n    def __init__(self, base_dir=None, limit_length=False, dev=False, test=False, no_testset=False, no_normalizers=False):\n\n        self.text_align_directory = FLAGS.text_align_directory\n\n        if no_testset:\n            devset = []\n            testset = []\n        else:\n            with open(FLAGS.testset_file) as f:\n                testset_json = json.load(f)\n                devset = testset_json['dev']\n                testset = testset_json['test']\n\n        directories = []\n        if base_dir is not None:\n            directories.append(EMGDirectory(0, base_dir, False))\n        else:\n            for sd in FLAGS.silent_data_directories:\n                for session_dir in sorted(os.listdir(sd)):\n                    directories.append(EMGDirectory(len(directories), os.path.join(sd, session_dir), True))\n\n            has_silent = len(FLAGS.silent_data_directories) > 0\n            for vd in FLAGS.voiced_data_directories:\n                for session_dir in sorted(os.listdir(vd)):\n                    directories.append(EMGDirectory(len(directories), os.path.join(vd, session_dir), False, exclude_from_testset=has_silent))\n\n        self.example_indices = []\n        self.voiced_data_locations = {} # map from book/sentence_index to directory_info/index\n        for directory_info in directories:\n            for fname in os.listdir(directory_info.directory):\n                m = re.match(r'(\\d+)_info.json', fname)\n                if m is not None:\n                    idx_str = m.group(1)\n                    with open(os.path.join(directory_info.directory, fname)) as f:\n                        info = json.load(f)\n                        if info['sentence_index'] >= 0: # boundary clips of silence are marked -1\n                            location_in_testset = [info['book'], info['sentence_index']] in testset\n                            location_in_devset = [info['book'], info['sentence_index']] in devset\n                            if (test and location_in_testset and not directory_info.exclude_from_testset) \\\n                                    or (dev and location_in_devset and not directory_info.exclude_from_testset) \\\n                                    or (not test and not dev and not location_in_testset and not location_in_devset):\n                                self.example_indices.append((directory_info,int(idx_str)))\n\n                            if not directory_info.silent:\n                                location = (info['book'], info['sentence_index'])\n                                self.voiced_data_locations[location] = (directory_info,int(idx_str))\n\n        self.example_indices.sort()\n        random.seed(0)\n        random.shuffle(self.example_indices)\n\n        self.no_normalizers = no_normalizers\n        if not self.no_normalizers:\n            self.mfcc_norm, self.emg_norm = pickle.load(open(FLAGS.normalizers_file,'rb'))\n\n        sample_mfccs, sample_emg, _, _, _, _ = load_utterance(self.example_indices[0][0].directory, self.example_indices[0][1])\n        self.num_speech_features = sample_mfccs.shape[1]\n        self.num_features = sample_emg.shape[1]\n        self.limit_length = limit_length\n        self.num_sessions = len(directories)\n\n        self.text_transform = TextTransform()\n\n    def silent_subset(self):\n        result = copy(self)\n        silent_indices = []\n        for example in self.example_indices:\n            if example[0].silent:\n                silent_indices.append(example)\n        result.example_indices = silent_indices\n        return result\n\n    def subset(self, fraction):\n        result = copy(self)\n        result.example_indices = self.example_indices[:int(fraction*len(self.example_indices))]\n        return result\n\n    def __len__(self):\n        return len(self.example_indices)\n\n    @lru_cache(maxsize=None)\n    def __getitem__(self, i):\n        directory_info, idx = self.example_indices[i]\n        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)\n        raw_emg = raw_emg / 20\n        raw_emg = 50*np.tanh(raw_emg/50.)\n\n        if not self.no_normalizers:\n            mfccs = self.mfcc_norm.normalize(mfccs)\n            emg = self.emg_norm.normalize(emg)\n            emg = 8*np.tanh(emg/8.)\n\n        session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)\n        audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'\n\n        text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)\n\n        result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}\n\n        if directory_info.silent:\n            voiced_directory, voiced_idx = self.voiced_data_locations[book_location]\n            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)\n\n            if not self.no_normalizers:\n                voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)\n                voiced_emg = self.emg_norm.normalize(voiced_emg)\n                voiced_emg = 8*np.tanh(voiced_emg/8.)\n\n            result['parallel_voiced_audio_features'] = torch.from_numpy(voiced_mfccs).pin_memory()\n            result['parallel_voiced_emg'] = torch.from_numpy(voiced_emg).pin_memory()\n\n            audio_file = f'{voiced_directory.directory}/{voiced_idx}_audio_clean.flac'\n\n        result['phonemes'] = torch.from_numpy(phonemes).pin_memory() # either from this example if vocalized or aligned example if silent\n        result['audio_file'] = audio_file\n\n        return result\n\n    @staticmethod\n    def collate_raw(batch):\n        batch_size = len(batch)\n        audio_features = []\n        audio_feature_lengths = []\n        parallel_emg = []\n        for ex in batch:\n            if ex['silent']:\n                audio_features.append(ex['parallel_voiced_audio_features'])\n                audio_feature_lengths.append(ex['parallel_voiced_audio_features'].shape[0])\n                parallel_emg.append(ex['parallel_voiced_emg'])\n            else:\n                audio_features.append(ex['audio_features'])\n                audio_feature_lengths.append(ex['audio_features'].shape[0])\n                parallel_emg.append(np.zeros(1))\n        phonemes = [ex['phonemes'] for ex in batch]\n        emg = [ex['emg'] for ex in batch]\n        raw_emg = [ex['raw_emg'] for ex in batch]\n        session_ids = [ex['session_ids'] for ex in batch]\n        lengths = [ex['emg'].shape[0] for ex in batch]\n        silent = [ex['silent'] for ex in batch]\n        text_ints = [ex['text_int'] for ex in batch]\n        text_lengths = [ex['text_int'].shape[0] for ex in batch]\n\n        result = {'audio_features':audio_features,\n                  'audio_feature_lengths':audio_feature_lengths,\n                  'emg':emg,\n                  'raw_emg':raw_emg,\n                  'parallel_voiced_emg':parallel_emg,\n                  'phonemes':phonemes,\n                  'session_ids':session_ids,\n                  'lengths':lengths,\n                  'silent':silent,\n                  'text_int':text_ints,\n                  'text_int_lengths':text_lengths}\n        return result\n\ndef make_normalizers():\n    dataset = EMGDataset(no_normalizers=True)\n    mfcc_samples = []\n    emg_samples = []\n    for d in dataset:\n        mfcc_samples.append(d['audio_features'])\n        emg_samples.append(d['emg'])\n        if len(emg_samples) > 50:\n            break\n    mfcc_norm = FeatureNormalizer(mfcc_samples, share_scale=True)\n    emg_norm = FeatureNormalizer(emg_samples, share_scale=False)\n    pickle.dump((mfcc_norm, emg_norm), open(FLAGS.normalizers_file, 'wb'))\n\nif __name__ == '__main__':\n    d = EMGDataset()\n    for i in range(1000):\n        d[i]\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:27:00.256062Z","iopub.execute_input":"2023-07-25T23:27:00.256531Z","iopub.status.idle":"2023-07-25T23:27:00.272538Z","shell.execute_reply.started":"2023-07-25T23:27:00.256498Z","shell.execute_reply":"2023-07-25T23:27:00.271610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/sooftware/conformer.git\n!cd conformer && pip install .\n\n!git clone --recursive https://github.com/parlance/ctcdecode.git\n!cd ctcdecode && pip install .","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:27:00.275725Z","iopub.execute_input":"2023-07-25T23:27:00.276456Z","iopub.status.idle":"2023-07-25T23:29:54.412649Z","shell.execute_reply.started":"2023-07-25T23:27:00.276411Z","shell.execute_reply":"2023-07-25T23:29:54.411475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom conformer import Conformer\nfrom torch import nn\nfrom typing import Tuple\n\nclass ConformerCTC(nn.Module):\n    def __init__(self,\n#                  freq_mask: int = 27,\n#                  time_mask_ratio: float = 0.05,\n                 **kwargs):\n        super(ConformerCTC, self).__init__()\n#         self.spec_aug = SpecAug(freq_mask, time_mask_ratio)\n        self.encoder = Conformer(**kwargs)\n    \n    def forward(self,\n                inputs: torch.Tensor,\n                input_length: torch.Tensor\n               ) -> Tuple[torch.Tensor, torch.Tensor]:\n#         inputs = self.spec_aug(inputs)\n        outputs, output_lengths = self.encoder(inputs, input_length)\n        return outputs, output_lengths\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:29:54.414773Z","iopub.execute_input":"2023-07-25T23:29:54.415175Z","iopub.status.idle":"2023-07-25T23:29:55.971024Z","shell.execute_reply.started":"2023-07-25T23:29:54.415139Z","shell.execute_reply":"2023-07-25T23:29:55.970056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\nPath('/kaggle/working/outputs').mkdir(parents=True, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:29:55.972340Z","iopub.execute_input":"2023-07-25T23:29:55.973540Z","iopub.status.idle":"2023-07-25T23:29:55.978821Z","shell.execute_reply.started":"2023-07-25T23:29:55.973511Z","shell.execute_reply":"2023-07-25T23:29:55.977808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_input(specs, device):\n    '''\n        specs: (batch, time step, feature)\n    '''\n    batch, time_step, _ = specs.size()\n    input_length = torch.full(size=(batch,),\n                             fill_value=time_step, dtype=torch.long)\n    return specs.to(device), input_length.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:31:07.647272Z","iopub.execute_input":"2023-07-25T23:31:07.647652Z","iopub.status.idle":"2023-07-25T23:31:07.654050Z","shell.execute_reply.started":"2023-07-25T23:31:07.647620Z","shell.execute_reply":"2023-07-25T23:31:07.652852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_target(transcript, device):\n    target_length = torch.LongTensor([i.size(0) for i in transcript])\n    #print(target_length)\n    target = torch.nn.utils.rnn.pad_sequence(transcript, batch_first=True)\n    return target.to(device), target_length.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:31:07.656200Z","iopub.execute_input":"2023-07-25T23:31:07.656784Z","iopub.status.idle":"2023-07-25T23:31:07.668788Z","shell.execute_reply.started":"2023-07-25T23:31:07.656751Z","shell.execute_reply":"2023-07-25T23:31:07.667633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport logging\nimport subprocess\nfrom ctcdecode import CTCBeamDecoder\nimport jiwer\nimport random\nimport librosa\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom read_emg import EMGDataset, SizeAwareSampler\nfrom data_utils import combine_fixed_length, decollate_tensor\n\nfrom absl import flags\nFLAGS = flags.FLAGS\n\nimport scipy.signal as signal\n\ngc.get_threshold()\ngc.get_count()\ngc.collect()\ngc.get_count()\n\ndef test(model, testset, device):\n    model.eval()\n\n    blank_id = len(testset.text_transform.chars)\n    decoder = CTCBeamDecoder(testset.text_transform.chars+'_', blank_id=blank_id, log_probs_input=True,\n            model_path='/kaggle/input/librispeech-4gram-language-model/4-gram-librispeech.bin', alpha=1.5, beta=1.85)\n\n    dataloader = torch.utils.data.DataLoader(testset, batch_size=1)\n    references = []\n    predictions = []\n    \n    \n    n_chars = len(testset.text_transform.chars)\n    spec = testset.num_features\n    \n    hyp = dict(\n    num_classes=n_chars+1,\n    input_dim=spec,\n    encoder_dim=144,\n    num_encoder_layers=16,\n    num_attention_heads=4,\n    conv_kernel_size=31)\n    \n    model = ConformerCTC(**hyp).to(device)\n    \n    with torch.no_grad():\n        for example in dataloader:\n            X = example['emg'].to(device)\n            X_raw = example['raw_emg'].to(device)\n            sess = example['session_ids'].to(device)\n\n            inputs, input_lengths = make_input(X, device)\n            pred, preds_length  = model(inputs, input_lengths)\n            #pred = F.log_softmax(pred, -1)\n            \n\n            beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)\n            pred_int = beam_results[0,0,:out_lens[0,0]].tolist()\n\n            pred_text = testset.text_transform.int_to_text(pred_int)\n            #print('pred_text: ', pred_text)\n            target_text = testset.text_transform.clean_text(example['text'][0])\n            #print('target text: ',target_text)\n            print(len(target_text))\n            \n            if len(target_text) != 0:\n                references.append(target_text)\n                #print('references: ', references)\n                predictions.append(pred_text)\n                #print('predictions: ', predictions)\n                #print(len(references))\n            else:\n                continue\n    model.train()\n    print('references: ', references)\n    print('predictions: ', predictions)\n    return jiwer.wer(references, predictions)\n\ndef pad_target(target, expected_length, blank_symbol):\n    # Verifica o tamanho atual do alvo (target)\n    current_length = target.size(0)\n\n    # Se o tamanho atual já for igual ao esperado, não é necessário fazer o preenchimento\n    if current_length == expected_length:\n        return target\n\n    # Calcula quantos símbolos em branco precisam ser adicionados ao alvo\n    num_blanks = expected_length - current_length\n\n    # Transpõe o tensor para que o preenchimento seja aplicado na dimensão correta\n    transposed_target = target.transpose(0, 1)\n\n    # Faz o preenchimento do tensor de destino\n    padded_target = F.pad(transposed_target, (0, num_blanks), value=blank_symbol)\n\n    # Transpõe o tensor de volta para sua forma original\n    padded_target = padded_target.transpose(0, 1)\n\n    return padded_target\n\n\ndef train_model(trainset, devset, device, n_epochs=200):\n    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))\n\n    n_chars = len(devset.text_transform.chars)\n    spec = devset.num_features\n    \n    hyp = dict(\n    num_classes=n_chars+1,\n    input_dim=spec,\n    encoder_dim=144,\n    num_encoder_layers=16,\n    num_attention_heads=4,\n    conv_kernel_size=31)\n    \n    model = ConformerCTC(**hyp).to(device)\n\n    if FLAGS.start_training_from is not None:\n        state_dict = torch.load(FLAGS.start_training_from)\n        model.load_state_dict(state_dict, strict=False)\n\n    optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)\n    lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)\n\n    def set_lr(new_lr):\n        for param_group in optim.param_groups:\n            param_group['lr'] = new_lr\n\n    target_lr = FLAGS.learning_rate\n    def schedule_lr(iteration):\n        iteration = iteration + 1\n        if iteration <= FLAGS.learning_rate_warmup:\n            set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)\n\n    batch_idx = 0\n    optim.zero_grad()\n    for epoch_idx in range(n_epochs):\n        gc.collect()\n        gc.get_count()\n        losses = []\n        max_target_length = 0\n        all_y = []\n        for example in dataloader:\n            schedule_lr(batch_idx)\n            \n            X = combine_fixed_length(example['emg'], 200).to(device)\n            sess = combine_fixed_length(example['session_ids'], 200).to(device)\n            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)\n            \n            inputs, input_lengths = make_input(X, device)\n            pred, pred_length = model(inputs, input_lengths)\n\n            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, pred_length), batch_first=False)\n            y = example['text_int']\n            target, target_lengths = make_target(y, device)\n            \n            gc.get_count()\n            gc.collect()\n            gc.get_count()\n\n            expected_batch_size = pred.size(1)   # Obtém o tamanho do lote esperado a partir do tensor pred\n\n            #print(\"Tamanho de target_lengths:\", len(target_lengths))\n            #print(\"Tamanho esperado do lote:\", expected_batch_size)\n\n            # Verifica se o tamanho do lote (batch size) do tensor target_lengths é igual ao esperado\n            if len(target_lengths) != expected_batch_size:\n                # Ajusta o tamanho do tensor target_lengths para corresponder ao tamanho do lote esperado\n                target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.long, device=device)\n                #print(\"Tamanho de target_lengths_tensor antes:\", target_lengths_tensor.size())\n\n                # Calcula o número de elementos a serem preenchidos com zeros\n                num_elements_to_pad = expected_batch_size - len(target_lengths)\n\n                # Preenche o tensor com zeros para igualar ao tamanho do lote esperado\n                target_lengths_tensor = F.pad(target_lengths_tensor, (0, num_elements_to_pad), value=0)\n                #print(\"Tamanho de target_lengths_tensor depois:\", target_lengths_tensor.size())\n              \n    \n            device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n            target = target.to(device)\n            \n            blank_symbol = 0  # Replace 0 with the appropriate blank symbol index\n            padded_target = pad_target(target, expected_batch_size, blank_symbol)\n            \n            #print(\"Tamanho de pred:\", pred.size())\n            #print(\"Tamanho de target:\", target.size())\n            #print(\"Tamanho de padded_target:\", padded_target.size())\n            #print(\"Tamanho de pred_length:\", pred_length.size())\n            #print(\"Tamanho de target_lengths_tensor:\", target_lengths_tensor.size())\n\n    \n            cal_loss = nn.CTCLoss(zero_infinity=True).to(device)\n            loss = cal_loss(pred, padded_target, pred_length, target_lengths_tensor)\n            gc.collect()\n            print(loss)\n\n\n            losses.append(loss.item())\n\n            loss.backward()\n            if (batch_idx+1) % 2 == 0:\n                optim.step()\n                optim.zero_grad()\n\n            batch_idx += 1\n        train_loss = np.mean(losses)\n        val = test(model, devset, device)\n        lr_sched.step()\n        logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')\n        print(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')\n        torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))\n\n    model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters\n    return model\n\ndef evaluate_saved():\n    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n    testset = EMGDataset(test=True)\n    \n    n_chars = len(testset.text_transform.chars)\n    spec = testset.num_features\n    \n    hyp = dict(\n    num_classes=n_chars+1,\n    input_dim=spec,\n    encoder_dim=144,\n    num_encoder_layers=16,\n    num_attention_heads=4,\n    conv_kernel_size=31)\n    \n    model = ConformerCTC(**hyp).to(device)\n    model.load_state_dict(torch.load(FLAGS.evaluate_saved))\n    print('WER:', test(model, testset, device))\n\ndef main():\n    os.makedirs(FLAGS.output_directory, exist_ok=True)\n    logging.basicConfig(handlers=[\n            logging.FileHandler(os.path.join(FLAGS.output_directory, 'log.txt'), 'w'),\n            logging.StreamHandler()\n            ], level=logging.INFO, format=\"%(message)s\")\n\n    logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)\n    logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)\n\n    logging.info(sys.argv)\n\n    trainset = EMGDataset(dev=False,test=False).subset(0.8)\n    devset = EMGDataset(dev=True)\n    logging.info('output example: %s', devset.example_indices[0])\n    logging.info('train / dev split: %d %d',len(trainset),len(devset))\n\n    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n    \n    \n    model = train_model(trainset, devset, device)\n\nif __name__ == '__main__':\n    if FLAGS.evaluate_saved is not None:\n        evaluate_saved()\n    else:\n        main()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T23:31:07.715844Z","iopub.execute_input":"2023-07-25T23:31:07.716126Z"},"trusted":true},"execution_count":null,"outputs":[]}]}