{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install praat-textgrids\n!pip install jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom absl import flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('normalizers_file', '/kaggle/input/normalizers/normalizers.pkl', 'file with pickled feature normalizers')\n\nflags.DEFINE_list('remove_channels', [], 'channels to remove')\nflags.DEFINE_list('silent_data_directories', ['/kaggle/input/emgdata/emg_data/silent_parallel_data'], 'silent data locations')\nflags.DEFINE_list('voiced_data_directories', ['/kaggle/input/emgdata/emg_data/voiced_parallel_data','/kaggle/input/emgdata/emg_data/nonparallel_data'], 'voiced data locations')\nflags.DEFINE_string('testset_file', '/kaggle/input/testesetlarge/testset_largedev.json', 'file with testset indices')\nflags.DEFINE_string('text_align_directory', '/kaggle/input/textaligns/text_alignments', 'directory with alignment files')\n\nflags.DEFINE_boolean('debug', False, 'debug')\nflags.DEFINE_string('output_directory', '/kaggle/working/outputs', 'where to save models and outputs')\nflags.DEFINE_integer('batch_size', 16, 'training batch size')\nflags.DEFINE_float('learning_rate', 3e-4, 'learning rate')\nflags.DEFINE_integer('learning_rate_warmup', 1000, 'steps of linear warmup')\nflags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')\nflags.DEFINE_string('start_training_from', None, 'start training from this model')\nflags.DEFINE_float('l2', 0, 'weight decay')\nflags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')\nflags.DEFINE_integer('accumulation_steps', 4, 'número debatches para acumular gradientes')\n\nFLAGS(sys.argv[1:])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile data_utils.py\n\nimport string\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom textgrids import TextGrid\nimport jiwer\nfrom unidecode import unidecode\n\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom absl import flags\nFLAGS = flags.FLAGS\n\nphoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']\n\ndef normalize_volume(audio): #recebe um sinal de áudio e realiza uma normalização de volume nele.\n    rms = librosa.feature.rms(audio)  #calcula o valor RMS (root mean square) do sinal de áudio \n    max_rms = rms.max() + 0.01\n    target_rms = 0.2\n    audio = audio * (target_rms/max_rms) #normaliza o sinal de áudio multiplicando-o por um fator que ajusta o RMS para um valor de destino (0.2)\n    max_val = np.abs(audio).max()\n    if max_val > 1.0: # this shouldn't happen too often with the target_rms of 0.2\n        audio = audio / max_val  #Se o valor máximo absoluto do sinal de áudio for maior que 1.0, o sinal é dividido pelo valor máximo para evitar a saturação.\n    return audio\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):  #realiza uma compressão de faixa dinâmica em um tensor\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\ndef spectral_normalize_torch(magnitudes):  #realiza a normalização espectral\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\nmel_basis = {}\nhann_window = {}\n\ndef mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):  # calcula o espectrograma mel de um sinal de áudio\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    if fmax not in mel_basis: #verificam se já foi calculada a base de mel correspondente à frequência máxima fmax. Se não tiver sido calculada, a função librosa.filters.mel \n                                #é usada para calcular a base de mel com os parâmetros fornecidos\n        mel = librosa.filters.mel(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n\n        #print('Mel ', mel)\n        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n        #print('Mel_basis ', mel_basis)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect') #preenche o sinal de áudio y com reflexão antes do cálculo do espectrograma. \n                                                                                #O sinal é expandido com uma dimensão extra e é aplicado um preenchimento refletivo em ambas as extremidades.\n    y = y.squeeze(1) #a dimensão extra é removida.\n    #print('y ', y)\n    #spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)], #calcula a transformada de Fourier de curto tempo (STFT) do sinal de áudio y\n                      #center=center, pad_mode='reflect', normalized=False, onesided=True)  #O espectrograma é calculado apenas para a metade positiva das frequências (onesided=True).\n    #print ('Resutado do stft ', spec)\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n                  center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n    #print ('Resutado do stft ', spec)\n    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9)) #Essa linha calcula o módulo (magnitude) do espectrograma. O espectrograma é elevado ao quadrado, somado ao valor de 1e-9 para evitar divisão por zero\n    # Imprimir as dimensões de mel_basis\n    #print ('Resutado do sqrt ', spec)\n    #print(len(mel_basis))\n    #for key, value in mel_basis.items():\n        #print(f\"Dimensões de {key}: {value.shape}\")\n    #print('Dimensões de spec ', spec.shape)\n    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n\n    \n    #spec = mel_basis[str(fmax)+'_'+str(y.device)] @ spec\n\n    #print ('Resutado da multiplicação ', spec)\n    #print('Dimensões de spec ', spec.shape)\n    spec = spectral_normalize_torch(spec) #O resultado é normalizado espectralmente\n\n    return spec\n\ndef load_audio(filename, start=None, end=None, max_frames=None, renormalize_volume=False): #carrega um arquivo de áudio, realiza pré-processamento nele e retorna o espectrograma mel correspondente\n    audio, r = sf.read(filename)\n\n    if len(audio.shape) > 1:\n        audio = audio[:,0] # verifica se o sinal de áudio possui mais de uma dimensão, o que indicaria que é um áudio estéreo. Nesse caso, seleciona apenas o primeiro canal de áudio.\n    if start is not None or end is not None:\n        audio = audio[start:end] #permitem selecionar uma parte específica do sinal de áudio\n\n    if renormalize_volume:\n        audio = normalize_volume(audio)\n    if r == 16000:\n        audio = librosa.resample(audio, orig_sr=16000, target_sr=22050)  #verificam a taxa de amostragem r do áudio. Se for igual a 16000, o áudio é ressampleado para a taxa de amostragem de 22050 Hz usando a função librosa.resample. \n    else:\n        assert r == 22050\n    audio = np.clip(audio, -1, 1) # realiza um ajuste no intervalo de valores do sinal de áudio. Por vezes, o ressampleamento pode fazer com que alguns valores ultrapassem o intervalo [-1, 1], \n                                    #então essa linha garante que todos os valores estejam dentro desse intervalo.\n    pytorch_mspec = mel_spectrogram(torch.tensor(audio, dtype=torch.float32).unsqueeze(0), 1024, 80, 22050, 256, 1024, 0, 8000, center=False)  #calculam o espectrograma mel do sinal de áudio\n    mspec = pytorch_mspec.squeeze(0).T.numpy() #O resultado é convertido em uma matriz numpy e atribuído a mspec.\n    if max_frames is not None and mspec.shape[0] > max_frames:  #verifica se o parâmetro max_frames foi fornecido e se o número de quadros (linhas) do espectrograma excede max_frames\n        mspec = mspec[:max_frames,:] #o espectrograma é recortado para ter no máximo max_frames quadros.\n    return mspec\n\ndef double_average(x):  #suaviza um sinal x aplicando uma média móvel dupla, ou seja, realizando duas convoluções consecutivas com um filtro médio. Isso ajuda a reduzir o ruído e suavizar o sinal.\n    assert len(x.shape) == 1  # verificam se o sinal x tem apenas uma dimensão. Caso contrário, é lançado um erro.\n    f = np.ones(9)/9.0  #cria uma matriz de tamanho 9 preenchida com o valor 1.0 e, em seguida, divide todos os elementos por 9.0. Essa matriz f será usada como um filtro médio.\n    v = np.convolve(x, f, mode='same')  #aplica a convolução entre o sinal x e o filtro f. A opção mode='same' garante que o tamanho da saída seja o mesmo que o tamanho de x\n    w = np.convolve(v, f, mode='same')  #aplica a convolução entre o sinal v e o filtro f. A opção mode='same' garante que o tamanho da saída seja o mesmo que o tamanho de x\n    return w\n\ndef get_emg_features(emg_data, debug=False): #calcula recursos relacionados ao sinal de eletromiografia (EMG) a partir dos dados de EMG fornecido. Esses recursos incluem médias de janelas, valores RMS, \n                                    #taxa de cruzamento por zero e espectrograma de curto prazo. Esses recursos podem ser usados posteriormente para análise e processamento adicional dos sinais de EMG.\n    xs = emg_data - emg_data.mean(axis=0, keepdims=True)  #calculam xs, que é o sinal de EMG centrado em torno da média. \n                                                            #É subtraída a média de cada coluna dos dados de EMG (emg_data) usando a função mean ao longo do eixo 0.\n    frame_features = []\n    for i in range(emg_data.shape[1]):\n        x = xs[:,i]  # x: coluna atual de xs\n        w = double_average(x) # w: sinal resultante da aplicação da função double_average em x, ou seja, o sinal suavizado\n        p = x - w # p: sinal resultante da subtração de w de x, representando as partes pulsativas do sinal\n        r = np.abs(p) #r: sinal resultante do valor absoluto de p, representando a magnitude das partes pulsativas do sinal\n\n        w_h = librosa.util.frame(w, frame_length=16, hop_length=6).mean(axis=0) # w_h: média das janelas de 16 amostras de w com um deslocamento de 6 amostras\n        p_w = librosa.feature.rms(y=w, frame_length=16, hop_length=6, center=False)  #p_w: valor RMS (Root Mean Square) das janelas de 16 amostras de w com um deslocamento de 6 amostras\n        p_w = np.squeeze(p_w, 0)\n        p_r = librosa.feature.rms(y=r, frame_length=16, hop_length=6, center=False)  #p_r: valor RMS das janelas de 16 amostras de r com um deslocamento de 6 amostras\n        p_r = np.squeeze(p_r, 0)\n        z_p = librosa.feature.zero_crossing_rate(p, frame_length=16, hop_length=6, center=False) #z_p: taxa de cruzamento por zero das janelas de 16 amostras de p com um deslocamento de 6 amostras\n        z_p = np.squeeze(z_p, 0)\n        r_h = librosa.util.frame(r, frame_length=16, hop_length=6).mean(axis=0) #r_h: média das janelas de 16 amostras de r com um deslocamento de 6 amostras\n\n        s = abs(librosa.stft(np.ascontiguousarray(x), n_fft=16, hop_length=6, center=False))  #calcula o espectrograma de curto prazo do sinal x usando a Transformada de Fourier de Curto Prazo (STFT)\n        # s has feature dimension first and time second\n\n        if debug:\n            plt.subplot(7,1,1)\n            plt.plot(x)\n            plt.subplot(7,1,2)\n            plt.plot(w_h)\n            plt.subplot(7,1,3)\n            plt.plot(p_w)\n            plt.subplot(7,1,4)\n            plt.plot(p_r)\n            plt.subplot(7,1,5)\n            plt.plot(z_p)\n            plt.subplot(7,1,6)\n            plt.plot(r_h)\n\n            plt.subplot(7,1,7)\n            plt.imshow(s, origin='lower', aspect='auto', interpolation='nearest')\n\n            plt.show()\n\n        frame_features.append(np.stack([w_h, p_w, p_r, z_p, r_h], axis=1))  #empilham os recursos calculados para cada coluna em uma lista. Os recursossão empilhados verticalmente, resultando em uma matriz 2D.\n        frame_features.append(s.T) #o espectrograma s é transposto e adicionado à lista frame_features.\n\n    frame_features = np.concatenate(frame_features, axis=1) #concatena todos os elementos da lista frame_features ao longo do eixo 1, resultando em uma matriz unidimensional final\n    return frame_features.astype(np.float32)\n\nclass FeatureNormalizer(object):  #implementa um normalizador de recursos.Ela fornece métodos para normalizar uma amostra de recurso e desfazer a normalização, \n                #aplicando as médias e os desvios padrão calculados. Esse normalizador pode ser útil para preparar os dados antes de usá-los em um modelo de aprendizado de máquina.\n    def __init__(self, feature_samples, share_scale=False):#é o construtor da classe. Ele recebe uma lista de amostras de recursos (feature_samples), que são matrizes 2D com dimensões (tempo, recurso). \n        \"\"\" features_samples should be list of 2d matrices with dimension (time, feature) \"\"\"\n        feature_samples = np.concatenate(feature_samples, axis=0)  #as amostras de recursos são concatenadas ao longo do eixo 0 para formar uma única matriz\n        self.feature_means = feature_samples.mean(axis=0, keepdims=True)  #as médias dos recursos são calculadas ao longo do eixo 0 e armazenadas em self.feature_means\n        if share_scale:\n            self.feature_stddevs = feature_samples.std()  #Se share_scale for True, o desvio padrão de todos os recursos é calculado e armazenado em self.feature_stddevs\n        else:\n            self.feature_stddevs = feature_samples.std(axis=0, keepdims=True) #Caso contrário, os desvios padrão de cada recurso são calculados separadamente e armazenados em self.feature_stddevs.\n\n    def normalize(self, sample): #recebe uma amostra de recurso (sample) e normaliza essa amostra subtraindo as médias dos recursos (self.feature_means) e dividindo pelo desvio padrão dos recursos\n        sample -= self.feature_means\n        sample /= self.feature_stddevs\n        return sample\n\n    def inverse(self, sample): #ecebe uma amostra de recurso normalizada e realiza a operação inversa da normalização. Primeiro, a amostra é multiplicada pelo desvio padrão dos recursos. \n                                #Em seguida, a média dos recursos (self.feature_means) é adicionada de volta à amostra. \n        sample = sample * self.feature_stddevs\n        sample = sample + self.feature_means\n        return sample\n\ndef combine_fixed_length(tensor_list, length): #combina uma lista de tensores em um único tensor de comprimento fixo. \n                                #Ele garante que os dados sejam combinados em um tensor de tamanho fixo, preenchendo com zeros, se necessário.\n    total_length = sum(t.size(0) for t in tensor_list) #calcula o comprimento total somando o tamanho (dimensão 0) de cada tensor na lista.\n    if total_length % length != 0:#verifica se o comprimento total não é divisível pelo comprimento desejado\n        pad_length = length - (total_length % length)  #Se não for, calcula o comprimento de preenchimento necessário para tornar o total divisível pelo comprimento desejado\n        tensor_list = list(tensor_list) # copy\n        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device)) #um tensor preenchido com zeros é criado com o comprimento de \n                                                                #preenchimento necessário e as mesmas dimensões do primeiro tensor na lista. Esse tensor de preenchimento é anexado à lista de tensores.\n        total_length += pad_length \n    tensor = torch.cat(tensor_list, 0)  #os tensores na lista (incluindo o tensor de preenchimento, se adicionado) são concatenados ao longo da dimensão 0 para formar um único tensor.\n    n = total_length // length #o número de segmentos de comprimento fixo que podem ser extraídos do tensor é calculado dividindo o comprimento total pelo comprimento desejado.\n    return tensor.view(n, length, *tensor.size()[1:])  #o tensor é redimensionado para ter as dimensões (n, length, ...), onde n é o número de segmentos e ... representa as dimensões restantes dos tensores originais.\n\ndef decollate_tensor(tensor, lengths):  #desagrega um tensor em uma lista de tensores com comprimentos diferentes. Essa função é útil quando você deseja separar um tensor em segmentos de comprimentos diferentes, \n                                    #conforme especificado por uma lista de comprimentos. Pode ser usado, por exemplo, para processar lotes de dados em tamanhos diferentes após a etapa de inferência em um modelo.\n    b, s, d = tensor.size()  # obtem as dimensões do tensor original. b representa o tamanho do lote, s representa o comprimento dos segmentos e d representa a dimensão dos recursos.\n    tensor = tensor.view(b*s, d) # o tensor é redimensionado usando .view() para ter uma forma de (b * s, d). Isso combina o tamanho do lote com o comprimento dos segmentos.\n    results = []\n    idx = 0\n    for length in lengths: #Para cada comprimento, é verificado se a posição atual mais o comprimento está dentro dos limites do tensor. Se não estiver, um erro de assert é acionado.\n        assert idx + length <= b * s\n        results.append(tensor[idx:idx+length]) #o segmento correspondente é extraído do tensor, começando na posição idx e com o comprimento especificado. O segmento é adicionado à lista results.\n        idx += length\n    return results\n\ndef splice_audio(chunks, overlap): #combina várias partes de áudio sobrepostas em um único áudio.Essa função é útil para combinar partes de áudio sobrepostas, como segmentos de áudio em uma gravação \n                                        #contínua, onde a sobreposição ajuda a suavizar a transição entre as partes.\n    chunks = [c.copy() for c in chunks] # copy so we can modify in place\n\n    assert np.all([c.shape[0]>=overlap for c in chunks]) #é verificado se todas as partes de áudio têm um tamanho maior ou igual à sobreposição especificada. Isso é importante para garantir \n                                                            #que haja dados suficientes para aplicar a sobreposição corretamente.\n\n    result_len = sum(c.shape[0] for c in chunks) - overlap*(len(chunks)-1) #soma os tamanhos de todas as partes de áudio e subtraindo o tamanho da sobreposição entre as partes para o comprimento total\n    result = np.zeros(result_len, dtype=chunks[0].dtype)  #Um array de zeros chamado result é inicializado com o comprimento total calculado e o tipo de dados da primeira parte de áudio.\n\n    ramp_up = np.linspace(0,1,overlap) #Duas rampas, ramp_up e ramp_down, são criadas usando np.linspace() para representar as funções de aumento e diminuição gradual da amplitude durante a sobreposição.\n    ramp_down = np.linspace(1,0,overlap)\n\n    i = 0\n    for chunk in chunks:\n        l = chunk.shape[0]  #Em um loop for, cada parte de áudio é processada individualmente. Para cada parte de áudio, seu comprimento l é obtido.\n\n        # note: this will also fade the beginning and end of the result\n        chunk[:overlap] *= ramp_up  #As partes de áudio são multiplicadas pelos valores correspondentes nas rampas ramp_up e ramp_down. Isso aplica a sobreposição gradual no início e no final de cada parte de áudio.\n        chunk[-overlap:] *= ramp_down\n\n        result[i:i+l] += chunk #A parte de áudio processada é adicionada ao resultado final a partir da posição i. A variável i é atualizada para a próxima posição correta no resultado, levando em \n                                    #consideração o tamanho da parte de áudio e a sobreposição.\n        i += l-overlap\n\n    return result\n\ndef print_confusion(confusion_mat, n=10): #imprime informações sobre as confusões mais comuns em uma matriz de confusão. Essa função é útil para analisar e visualizar as confusões mais comuns em uma matriz de \n                            #confusão, fornecendo insights sobre o desempenho do modelo de classificação em relação a classes específicas.\n    # axes are (pred, target)\n    target_counts = confusion_mat.sum(0) + 1e-4  #calcula o número de ocorrências de cada classe alvo na matriz de confusão. Isso é feito somando os valores de cada coluna da matriz e adicionando um \n                                    #pequeno valor (1e-4) para evitar divisão por zero.\n    aslist = []\n    for p1 in range(len(phoneme_inventory)): #a função itera sobre todas as combinações únicas de classes alvo (p1 e p2). As confusões são calculadas somando as ocorrências nas células \n                                    #correspondentes na matriz de confusão e dividindo pelo número total de ocorrências das duas classes alvo.\n        for p2 in range(p1):\n            if p1 != p2:\n                aslist.append(((confusion_mat[p1,p2]+confusion_mat[p2,p1])/(target_counts[p1]+target_counts[p2]), p1, p2)) #As confusões são armazenadas em uma lista aslist como uma tupla contendo a taxa de confusão,\n                                                                                                                #o índice p1 da classe alvo, e o índice p2 da classe alvo.\n    aslist.sort() #classifica em ordem crescente com base na taxa de confusão\n    aslist = aslist[-n:]  #é selecionado o top n das confusões mais comuns.\n    max_val = aslist[-1][0]  #O valor máximo e o valor mínimo de confusão são obtidos a partir da lista aslist.\n    min_val = aslist[0][0]\n    val_range = max_val - min_val\n    print('Common confusions (confusion, accuracy)') \n    for v, p1, p2 in aslist:\n        p1s = phoneme_inventory[p1]\n        p2s = phoneme_inventory[p2]\n        print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')\n        #^ imprime as informações sobre as confusões mais comuns. Ela exibe a classe alvo p1, a classe alvo p2, a taxa de confusão (multiplicada por 100 para exibição em porcentagem) e a precisão (também \n        # multiplicada por 100).\n\ndef read_phonemes(textgrid_fname, max_len=None): #lê os fonemas de um arquivo TextGrid e os converte em uma sequência de índices de fonemas, usada para treinar e avaliar modelos de processamento de linguagem.\n    tg = TextGrid(textgrid_fname)\n    phone_ids = np.zeros(int(tg['phones'][-1].xmax*86.133)+1, dtype=np.int64)  #cria um array phone_ids preenchido com valores -1. O tamanho do array é calculado com base no tempo máximo encontrado \n            #no arquivo TextGrid multiplicado por 86.133. O valor 86.133 é usado para converter o tempo do TextGrid para a taxa de amostragem de 22050 Hz, que é a taxa de amostragem mencionada anteriormente.\n    phone_ids[:] = -1  \n    phone_ids[-1] = phoneme_inventory.index('sil') #o último valor do array é definido como o índice do fonema 'sil' no inventário de fonemas. Isso garante que a lista seja longa o suficiente para cobrir \n                                                    #todo o comprimento da sequência original.\n    for interval in tg['phones']: #percorre cada intervalo de fonema no arquivo TextGrid e mapeia o fonema correspondente para o seu índice no inventário de fonemas.\n        phone = interval.text.lower() \n        if phone in ['', 'sp', 'spn']: # Se o fonema for uma string vazia, 'sp' ou 'spn', ele é substituído por 'sil'\n            phone = 'sil'\n        if phone[-1] in string.digits: #Se o fonema tiver um dígito no final, o dígito é removido.\n            phone = phone[:-1]\n        ph_id = phoneme_inventory.index(phone) #Os índices de fonemas são atribuídos aos intervalos de tempo correspondentes no array phone_ids.\n        phone_ids[int(interval.xmin*86.133):int(interval.xmax*86.133)] = ph_id\n    assert (phone_ids >= 0).all(), 'missing aligned phones' #verifica se todos os valores de phone_ids são não negativos (ou seja, todos os fonemas foram encontrados e mapeados corretamente). \n                                #Caso contrário, uma exceção é lançada indicando que há fonemas ausentes na sequência alinhada.\n\n    if max_len is not None:  #Se max_len for especificado, a sequência de fonemas é truncada para o comprimento máximo especificado\n        phone_ids = phone_ids[:max_len]\n        assert phone_ids.shape[0] == max_len #A função verifica se o comprimento da sequência truncada é igual a max_len.\n    return phone_ids\n\nclass TextTransform(object): #implementa transformações de texto úteis, como limpeza de texto, mapeamento de texto para sequências de números inteiros e mapeamento inverso de sequências de números inteiros para texto.\n    def __init__(self):\n        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])  #transformation é um objeto jiwer.Compose que encapsula uma série de transformações de texto, como remoção de pontuação \n                                #e conversão para minúsculas. Essas transformações são realizadas pelo pacote jiwer.\n        self.chars = string.ascii_lowercase+string.digits+' ' #chars é uma string que contém todos os caracteres permitidos no texto, incluindo letras minúsculas, dígitos e espaço em branco.\n\n    def clean_text(self, text): #Limpa o texto aplicando as transformações definidas.\n        text = unidecode(text)  #remove quaisquer caracteres acentuados ou diacríticos\n        text = self.transformation(text) #as transformações definidas em self.transformation são aplicadas\n        return text\n\n    def text_to_int(self, text): #Converte o texto em uma sequência de números inteiros\n        text = self.clean_text(text) #O texto é limpo usando o método clean_text\n        return [self.chars.index(c) for c in text] #cada caractere do texto limpo é mapeado para o seu índice correspondente em self.chars\n\n    def int_to_text(self, ints):  #Converte uma sequência de números inteiros em texto\n        return ''.join(self.chars[i] for i in ints)  #Cada número inteiro em ints é mapeado para o caractere correspondente em self.chars. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile read_emg.py\n\nimport re\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom collections import defaultdict\nimport scipy\nimport json\nimport copy\nimport sys\nimport pickle\nimport string\nimport logging\nfrom functools import lru_cache\nfrom copy import copy\n\n\n\nimport librosa\nimport soundfile as sf\n\nimport torch\n\nfrom data_utils import load_audio, get_emg_features, FeatureNormalizer, phoneme_inventory, read_phonemes, TextTransform\n\nfrom scipy import signal\n\nfrom absl import flags\nFLAGS = flags.FLAGS\n\ndef remove_drift(signal, fs):\n    b, a = scipy.signal.butter(3, 2, 'highpass', fs=fs)\n    return scipy.signal.filtfilt(b, a, signal)\n\ndef notch(signal, freq, sample_frequency):\n    b, a = scipy.signal.iirnotch(freq, 30, sample_frequency)\n    return scipy.signal.filtfilt(b, a, signal)\n\ndef notch_harmonics(signal, freq, sample_frequency):\n    for harmonic in range(1,8):\n        signal = notch(signal, freq*harmonic, sample_frequency)\n    return signal\n\ndef subsample(signal, new_freq, old_freq):\n    times = np.arange(len(signal))/old_freq\n    sample_times = np.arange(0, times[-1], 1/new_freq)\n    result = np.interp(sample_times, times, signal)\n    return result\n\ndef apply_to_all(function, signal_array, *args, **kwargs):\n    results = []\n    for i in range(signal_array.shape[1]):\n        results.append(function(signal_array[:,i], *args, **kwargs))\n    return np.stack(results, 1)\n\ndef load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):\n    index = int(index)\n    raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))\n    before = os.path.join(base_dir, f'{index-1}_emg.npy')\n    after = os.path.join(base_dir, f'{index+1}_emg.npy')\n    if os.path.exists(before):\n        raw_emg_before = np.load(before)\n    else:\n        raw_emg_before = np.zeros([0,raw_emg.shape[1]])\n    if os.path.exists(after):\n        raw_emg_after = np.load(after)\n    else:\n        raw_emg_after = np.zeros([0,raw_emg.shape[1]])\n\n    x = np.concatenate([raw_emg_before, raw_emg, raw_emg_after], 0)\n    x = apply_to_all(notch_harmonics, x, 60, 1000)\n    x = apply_to_all(remove_drift, x, 1000)\n    x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]\n    emg_orig = apply_to_all(subsample, x, 689.06, 1000)\n    x = apply_to_all(subsample, x, 516.79, 1000)\n    emg = x\n\n    for c in FLAGS.remove_channels:\n        emg[:,int(c)] = 0\n        emg_orig[:,int(c)] = 0\n\n    emg_features = get_emg_features(emg)\n\n    mfccs = load_audio(os.path.join(base_dir, f'{index}_audio_clean.flac'),\n            max_frames=min(emg_features.shape[0], 800 if limit_length else float('inf')))\n\n    if emg_features.shape[0] > mfccs.shape[0]:\n        emg_features = emg_features[:mfccs.shape[0],:]\n    assert emg_features.shape[0] == mfccs.shape[0]\n    emg = emg[6:6+6*emg_features.shape[0],:]\n    emg_orig = emg_orig[8:8+8*emg_features.shape[0],:]\n    assert emg.shape[0] == emg_features.shape[0]*6\n\n    with open(os.path.join(base_dir, f'{index}_info.json')) as f:\n        info = json.load(f)\n\n    sess = os.path.basename(base_dir)\n    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'\n    if os.path.exists(tg_fname):\n        phonemes = read_phonemes(tg_fname, mfccs.shape[0])\n    else:\n        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')\n\n    return mfccs, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)\n\nclass EMGDirectory(object):\n    def __init__(self, session_index, directory, silent, exclude_from_testset=False):\n        self.session_index = session_index\n        self.directory = directory\n        self.silent = silent\n        self.exclude_from_testset = exclude_from_testset\n\n    def __lt__(self, other):\n        return self.session_index < other.session_index\n\n    def __repr__(self):\n        return self.directory\n\nclass SizeAwareSampler(torch.utils.data.Sampler):\n    def __init__(self, emg_dataset, max_len):\n        self.dataset = emg_dataset\n        self.max_len = max_len\n\n    def __iter__(self):\n        indices = list(range(len(self.dataset)))\n        random.shuffle(indices)\n        batch = []\n        batch_length = 0\n        for idx in indices:\n            directory_info, file_idx = self.dataset.example_indices[idx]\n            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:\n                info = json.load(f)\n            if not np.any([l in string.ascii_letters for l in info['text']]):\n                continue\n            length = sum([emg_len for emg_len, _, _ in info['chunks']])\n            if length > self.max_len:\n                logging.warning(f'Warning: example {idx} cannot fit within desired batch length')\n            if length + batch_length > self.max_len:\n                yield batch\n                batch = []\n                batch_length = 0\n            batch.append(idx)\n            batch_length += length\n        # dropping last incomplete batch\n\nclass EMGDataset(torch.utils.data.Dataset):\n    def __init__(self, base_dir=None, limit_length=False, dev=False, test=False, no_testset=False, no_normalizers=False):\n\n        self.text_align_directory = FLAGS.text_align_directory\n\n        if no_testset:\n            devset = []\n            testset = []\n        else:\n            with open(FLAGS.testset_file) as f:\n                testset_json = json.load(f)\n                devset = testset_json['dev']\n                testset = testset_json['test']\n\n        directories = []\n        if base_dir is not None:\n            directories.append(EMGDirectory(0, base_dir, False))\n        else:\n            for sd in FLAGS.silent_data_directories:\n                for session_dir in sorted(os.listdir(sd)):\n                    directories.append(EMGDirectory(len(directories), os.path.join(sd, session_dir), True))\n\n            has_silent = len(FLAGS.silent_data_directories) > 0\n            for vd in FLAGS.voiced_data_directories:\n                for session_dir in sorted(os.listdir(vd)):\n                    directories.append(EMGDirectory(len(directories), os.path.join(vd, session_dir), False, exclude_from_testset=has_silent))\n\n        self.example_indices = []\n        self.voiced_data_locations = {} # map from book/sentence_index to directory_info/index\n        for directory_info in directories:\n            for fname in os.listdir(directory_info.directory):\n                m = re.match(r'(\\d+)_info.json', fname)\n                if m is not None:\n                    idx_str = m.group(1)\n                    with open(os.path.join(directory_info.directory, fname)) as f:\n                        info = json.load(f)\n                        if info['sentence_index'] >= 0: # boundary clips of silence are marked -1\n                            location_in_testset = [info['book'], info['sentence_index']] in testset\n                            location_in_devset = [info['book'], info['sentence_index']] in devset\n                            if (test and location_in_testset and not directory_info.exclude_from_testset) \\\n                                    or (dev and location_in_devset and not directory_info.exclude_from_testset) \\\n                                    or (not test and not dev and not location_in_testset and not location_in_devset):\n                                self.example_indices.append((directory_info,int(idx_str)))\n\n                            if not directory_info.silent:\n                                location = (info['book'], info['sentence_index'])\n                                self.voiced_data_locations[location] = (directory_info,int(idx_str))\n\n        self.example_indices.sort()\n        random.seed(0)\n        random.shuffle(self.example_indices)\n\n        self.no_normalizers = no_normalizers\n        if not self.no_normalizers:\n            self.mfcc_norm, self.emg_norm = pickle.load(open(FLAGS.normalizers_file,'rb'))\n\n        sample_mfccs, sample_emg, _, _, _, _ = load_utterance(self.example_indices[0][0].directory, self.example_indices[0][1])\n        self.num_speech_features = sample_mfccs.shape[1]\n        self.num_features = sample_emg.shape[1]\n        self.limit_length = limit_length\n        self.num_sessions = len(directories)\n\n        self.text_transform = TextTransform()\n\n    def silent_subset(self):\n        result = copy(self)\n        silent_indices = []\n        for example in self.example_indices:\n            if example[0].silent:\n                silent_indices.append(example)\n        result.example_indices = silent_indices\n        return result\n\n    def subset(self, fraction):\n        result = copy(self)\n        result.example_indices = self.example_indices[:int(fraction*len(self.example_indices))]\n        return result\n\n    def __len__(self):\n        return len(self.example_indices)\n\n    @lru_cache(maxsize=None)\n    def __getitem__(self, i):\n        directory_info, idx = self.example_indices[i]\n        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)\n        raw_emg = raw_emg / 20\n        raw_emg = 50*np.tanh(raw_emg/50.)\n\n        if not self.no_normalizers:\n            mfccs = self.mfcc_norm.normalize(mfccs)\n            emg = self.emg_norm.normalize(emg)\n            emg = 8*np.tanh(emg/8.)\n\n        session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)\n        audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'\n\n        text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)\n\n        result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}\n\n        if directory_info.silent:\n            voiced_directory, voiced_idx = self.voiced_data_locations[book_location]\n            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)\n\n            if not self.no_normalizers:\n                voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)\n                voiced_emg = self.emg_norm.normalize(voiced_emg)\n                voiced_emg = 8*np.tanh(voiced_emg/8.)\n\n            result['parallel_voiced_audio_features'] = torch.from_numpy(voiced_mfccs).pin_memory()\n            result['parallel_voiced_emg'] = torch.from_numpy(voiced_emg).pin_memory()\n\n            audio_file = f'{voiced_directory.directory}/{voiced_idx}_audio_clean.flac'\n\n        result['phonemes'] = torch.from_numpy(phonemes).pin_memory() # either from this example if vocalized or aligned example if silent\n        result['audio_file'] = audio_file\n\n        return result\n\n    @staticmethod\n    def collate_raw(batch):\n        batch_size = len(batch)\n        audio_features = []\n        audio_feature_lengths = []\n        parallel_emg = []\n        for ex in batch:\n            if ex['silent']:\n                audio_features.append(ex['parallel_voiced_audio_features'])\n                audio_feature_lengths.append(ex['parallel_voiced_audio_features'].shape[0])\n                parallel_emg.append(ex['parallel_voiced_emg'])\n            else:\n                audio_features.append(ex['audio_features'])\n                audio_feature_lengths.append(ex['audio_features'].shape[0])\n                parallel_emg.append(np.zeros(1))\n        phonemes = [ex['phonemes'] for ex in batch]\n        emg = [ex['emg'] for ex in batch]\n        raw_emg = [ex['raw_emg'] for ex in batch]\n        session_ids = [ex['session_ids'] for ex in batch]\n        lengths = [ex['emg'].shape[0] for ex in batch]\n        silent = [ex['silent'] for ex in batch]\n        text_ints = [ex['text_int'] for ex in batch]\n        text_lengths = [ex['text_int'].shape[0] for ex in batch]\n\n        result = {'audio_features':audio_features,\n                  'audio_feature_lengths':audio_feature_lengths,\n                  'emg':emg,\n                  'raw_emg':raw_emg,\n                  'parallel_voiced_emg':parallel_emg,\n                  'phonemes':phonemes,\n                  'session_ids':session_ids,\n                  'lengths':lengths,\n                  'silent':silent,\n                  'text_int':text_ints,\n                  'text_int_lengths':text_lengths}\n        return result\n\ndef make_normalizers():\n    dataset = EMGDataset(no_normalizers=True)\n    mfcc_samples = []\n    emg_samples = []\n    for d in dataset:\n        mfcc_samples.append(d['audio_features'])\n        emg_samples.append(d['emg'])\n        if len(emg_samples) > 50:\n            break\n    mfcc_norm = FeatureNormalizer(mfcc_samples, share_scale=True)\n    emg_norm = FeatureNormalizer(emg_samples, share_scale=False)\n    pickle.dump((mfcc_norm, emg_norm), open(FLAGS.normalizers_file, 'wb'))\n\nif __name__ == '__main__':\n    d = EMGDataset()\n    for i in range(1000):\n        d[i]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone --recursive https://github.com/parlance/ctcdecode.git\n!cd ctcdecode && pip install .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile convolution.py\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom typing import Tuple\n\n\nclass MaskCNN(nn.Module):\n    r\"\"\"\n    Masking Convolutional Neural Network\n\n    Adds padding to the output of the module based on the given lengths.\n    This is to ensure that the results of the model do not change when batch sizes change during inference.\n    Input needs to be in the shape of (batch_size, channel, hidden_dim, seq_len)\n\n    Refer to https://github.com/SeanNaren/deepspeech.pytorch/blob/master/model.py\n    Copyright (c) 2017 Sean Naren\n    MIT License\n\n    Args:\n        sequential (torch.nn): sequential list of convolution layer\n\n    Inputs: inputs, seq_lengths\n        - **inputs** (torch.FloatTensor): The input of size BxCxHxT\n        - **seq_lengths** (torch.IntTensor): The actual length of each sequence in the batch\n\n    Returns: output, seq_lengths\n        - **output**: Masked output from the sequential\n        - **seq_lengths**: Sequence length of output from the sequential\n    \"\"\"\n    def __init__(self, sequential: nn.Sequential) -> None:\n        super(MaskCNN, self).__init__()\n        self.sequential = sequential\n\n    def forward(self, inputs: Tensor, seq_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n        output = None\n\n        for module in self.sequential:\n            output = module(inputs)\n            mask = torch.BoolTensor(output.size()).fill_(0)\n            \n            if output.is_cuda:\n                mask = mask.cuda()\n            \n            print('mask size: ', mask.size())\n            \n            seq_lengths = self._get_sequence_lengths(module, seq_lengths)\n\n            length = seq_lengths\n\n            #if (mask[idx].size(2) - length) > 0:\n                #mask[idx].narrow(dim=2, start=length, length=mask[idx].size(2) - length).fill_(1)\n\n            output = output.masked_fill(mask, 0)\n            inputs = output\n            \n\n        return output, seq_lengths\n\n    def _get_sequence_lengths(self, module: nn.Module, seq_lengths: Tensor) -> Tensor:\n        r\"\"\"\n        Calculate convolutional neural network receptive formula\n\n        Args:\n            module (torch.nn.Module): module of CNN\n            seq_lengths (torch.IntTensor): The actual length of each sequence in the batch\n\n        Returns: seq_lengths\n            - **seq_lengths**: Sequence length of output from the module\n        \"\"\"\n        #print('seq_lengths 1: ', seq_lengths)\n        if isinstance(module, nn.Conv2d):\n            numerator = seq_lengths + 2 * module.padding[1] - module.dilation[1] * (module.kernel_size[1] - 1) - 1\n            seq_lengths = numerator / float(module.stride[1])\n            seq_lengths = round(seq_lengths) + 1\n            #print('seq_lengths 2: ', seq_lengths)\n\n        elif isinstance(module, nn.MaxPool2d):\n            seq_lengths >>= 1\n            #print('seq_lengths 3: ', seq_lengths)\n\n        return seq_lengths\n\n\nclass VGGExtractor(nn.Module):\n    r\"\"\"\n    VGG extractor for automatic speech recognition described in\n    \"Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM\" paper\n    - https://arxiv.org/pdf/1706.02737.pdf\n\n    Args:\n        input_dim (int): Dimension of input vector\n        in_channels (int): Number of channels in the input image\n        out_channels (int or tuple): Number of channels produced by the convolution\n\n    Inputs: inputs, input_lengths\n        - **inputs** (batch, time, dim): Tensor containing input vectors\n        - **input_lengths**: Tensor containing containing sequence lengths\n\n    Returns: outputs, output_lengths\n        - **outputs**: Tensor produced by the convolution\n        - **output_lengths**: Tensor containing sequence lengths produced by the convolution\n    \"\"\"\n    def __init__(\n            self,\n            input_dim: int,\n            in_channels: int = 1,\n            out_channels: int or tuple = (64, 128),\n    ):\n        super(VGGExtractor, self).__init__()\n        self.input_dim = input_dim\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.conv = MaskCNN(\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels[0], kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(num_features=out_channels[0]),\n                nn.ReLU(),\n                nn.Conv2d(out_channels[0], out_channels[0], kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(num_features=out_channels[0]),\n                nn.ReLU(),\n                nn.MaxPool2d(2, stride=2),\n                nn.Conv2d(out_channels[0], out_channels[1], kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(num_features=out_channels[1]),\n                nn.ReLU(),\n                nn.Conv2d(out_channels[1], out_channels[1], kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(num_features=out_channels[1]),\n                nn.ReLU(),\n                nn.MaxPool2d(2, stride=2),\n            )\n        )\n\n    def get_output_lengths(self, seq_lengths: Tensor):\n        assert self.conv is not None, \"self.conv should be defined\"\n\n        for module in self.conv:\n            if isinstance(module, nn.Conv2d):\n                numerator = seq_lengths + 2 * module.padding[1] - module.dilation[1] * (module.kernel_size[1] - 1) - 1\n                seq_lengths = numerator.float() / float(module.stride[1])\n                seq_lengths = seq_lengths.int() + 1\n\n            elif isinstance(module, nn.MaxPool2d):\n                seq_lengths >>= 1\n\n        return seq_lengths.int()\n\n    def get_output_dim(self):\n        return round(self.input_dim*2.29)\n\n    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n        r\"\"\"\n        inputs: torch.FloatTensor (batch, time, dimension)\n        input_lengths: torch.IntTensor (batch)\n        \"\"\"\n        outputs, output_lengths = self.conv(inputs.unsqueeze(1).transpose(2, 3), input_lengths)\n\n        batch_size, channels, dimension, seq_lengths = outputs.size()\n        outputs = outputs.permute(0, 3, 1, 2)\n        outputs = outputs.view(batch_size, seq_lengths, channels * dimension)\n\n        return outputs, output_lengths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile embeddings.py\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional Encoding proposed in \"Attention Is All You Need\".\n    Since speech_transformer contains no recurrence and no convolution, in order for the model to make\n    use of the order of the sequence, we must add some positional information.\n\n    \"Attention Is All You Need\" use sine and cosine functions of different frequencies:\n        PE_(pos, 2i)    =  sin(pos / power(10000, 2i / d_model))\n        PE_(pos, 2i+1)  =  cos(pos / power(10000, 2i / d_model))\n    \"\"\"\n    def __init__(self, d_model: int = 512, max_len: int = 5000) -> None:\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model, requires_grad=False)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, length: int) -> Tensor:\n        return self.pe[:, :length]\n\n\nclass Embedding(nn.Module):\n    \"\"\"\n    Embedding layer. Similarly to other sequence transduction models, speech_transformer use learned embeddings\n    to convert the input tokens and output tokens to vectors of dimension d_model.\n    In the embedding layers, speech_transformer multiply those weights by sqrt(d_model)\n    \"\"\"\n    def __init__(self, num_embeddings: int, pad_id: int, d_model: int = 512) -> None:\n        super(Embedding, self).__init__()\n        self.sqrt_dim = math.sqrt(d_model)\n        self.embedding = nn.Embedding(num_embeddings, d_model, padding_idx=pad_id)\n\n    def forward(self, inputs: Tensor) -> Tensor:\n        return self.embedding(inputs) * self.sqrt_dim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile mask.py\n\nimport torch\n\nfrom torch import Tensor\n\n\ndef get_attn_pad_mask(inputs, input_lengths, expand_length):\n    \"\"\" mask position is set to 1 \"\"\"\n    def get_transformer_non_pad_mask(inputs: Tensor, input_lengths: Tensor) -> Tensor:\n        \"\"\" Padding position is set to 0, either use input_lengths or pad_id \"\"\"\n        batch_size = inputs.size(0)\n\n        if isinstance(input_lengths, int):\n            input_lengths = torch.tensor([input_lengths] * batch_size, dtype=torch.long, device=inputs.device)\n\n        if len(inputs.size()) == 2:\n            non_pad_mask = inputs.new_ones(inputs.size())  # B x T\n        elif len(inputs.size()) == 3:\n            non_pad_mask = inputs.new_ones(inputs.size()[:-1])  # B x T\n        else:\n            raise ValueError(f\"Unsupported input shape {inputs.size()}\")\n\n        for i in range(batch_size):\n            length = input_lengths[i].item()  # Extrair o valor inteiro\n            non_pad_mask[i, length:] = 0\n\n        return non_pad_mask\n\n\n    non_pad_mask = get_transformer_non_pad_mask(inputs, input_lengths)\n    pad_mask = non_pad_mask.lt(1)\n    attn_pad_mask = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n    return attn_pad_mask\n\n\ndef get_attn_subsequent_mask(seq):\n    assert seq.dim() == 2\n    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)\n\n    if seq.is_cuda:\n        subsequent_mask = subsequent_mask.cuda()\n\n    return subsequent_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile modules.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nfrom torch import Tensor\nfrom typing import Tuple\n\n\nclass MaskConv2d(nn.Module):\n    \"\"\"\n    Masking Convolutional Neural Network\n    Adds padding to the output of the module based on the given lengths.\n    This is to ensure that the results of the model do not change when batch sizes change during inference.\n    Input needs to be in the shape of (batch_size, channel, hidden_dim, seq_len)\n    Refer to https://github.com/SeanNaren/deepspeech.pytorch/blob/master/model.py\n    Copyright (c) 2017 Sean Naren\n    MIT License\n    Args:\n        sequential (torch.nn): sequential list of convolution layer\n    Inputs: inputs, seq_lengths\n        - **inputs** (torch.FloatTensor): The input of size BxCxHxT\n        - **seq_lengths** (torch.IntTensor): The actual length of each sequence in the batch\n    Returns: output, seq_lengths\n        - **output**: Masked output from the sequential\n        - **seq_lengths**: Sequence length of output from the sequential\n    \"\"\"\n    def __init__(self, sequential: nn.Sequential) -> None:\n        super(MaskConv2d, self).__init__()\n        self.sequential = sequential\n\n    def forward(self, inputs: Tensor, seq_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n        output = None\n\n        for module in self.sequential:\n            output = module(inputs)\n            mask = torch.BoolTensor(output.size()).fill_(0)\n\n            if output.is_cuda:\n                mask = mask.cuda()\n\n            seq_lengths = self.get_sequence_lengths(module, seq_lengths)\n            length = seq_lengths\n\n            if (mask[idx].size(2) - length) > 0:\n                mask[idx].narrow(dim=2, start=length, length=mask[idx].size(2) - length).fill_(1)\n\n            output = output.masked_fill(mask, 0)\n            inputs = output\n\n        return output, seq_lengths\n\n    def get_sequence_lengths(self, module: nn.Module, seq_lengths: Tensor) -> Tensor:\n        \"\"\"\n        Calculate convolutional neural network receptive formula\n        Args:\n            module (torch.nn.Module): module of CNN\n            seq_lengths (torch.IntTensor): The actual length of each sequence in the batch\n        Returns: seq_lengths\n            - **seq_lengths**: Sequence length of output from the module\n        \"\"\"\n        if isinstance(module, nn.Conv2d):\n            numerator = seq_lengths + 2 * module.padding[1] - module.dilation[1] * (module.kernel_size[1] - 1) - 1\n            seq_lengths = numerator.float() / float(module.stride[1])\n            seq_lengths = seq_lengths.int() + 1\n\n        elif isinstance(module, nn.MaxPool2d):\n            seq_lengths >>= 1\n\n        return seq_lengths.int()\n\n\nclass Linear(nn.Module):\n    \"\"\"\n    Wrapper class of torch.nn.Linear\n    Weight initialize by xavier initialization and bias initialize to zeros.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n        super(Linear, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        init.xavier_uniform_(self.linear.weight)\n        if bias:\n            init.zeros_(self.linear.bias)\n\n    def forward(self, x: Tensor) -> Tensor:\n        #print('x: ', x)\n        #print('x size: ', x.size())\n        return self.linear(x)\n\n\nclass Transpose(nn.Module):\n    \"\"\" Wrapper class of torch.transpose() for Sequential module. \"\"\"\n    def __init__(self, shape: tuple):\n        super(Transpose, self).__init__()\n        self.shape = shape\n\n    def forward(self, inputs: Tensor):\n        return inputs.transpose(*self.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile sublayers.py\n\nimport torch.nn as nn\n\nfrom torch import Tensor\nfrom typing import Any, Optional\nfrom modules import (\n    Linear,\n    MaskConv2d,\n)\n\n\nclass AddNorm(nn.Module):\n    \"\"\"\n    Add & Normalization layer proposed in \"Attention Is All You Need\".\n    Transformer employ a residual connection around each of the two sub-layers,\n    (Multi-Head Attention & Feed-Forward) followed by layer normalization.\n    \"\"\"\n    def __init__(self, sublayer: nn.Module, d_model: int = 512) -> None:\n        super(AddNorm, self).__init__()\n        self.sublayer = sublayer\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, *args):\n        residual = args[0]\n        output = self.sublayer(*args)\n\n        if isinstance(output, tuple):\n            return self.layer_norm(output[0] + residual), output[1]\n\n        return self.layer_norm(output + residual)\n\n\nclass PositionWiseFeedForwardNet(nn.Module):\n    \"\"\"\n    Position-wise Feedforward Networks proposed in \"Attention Is All You Need\".\n    Fully connected feed-forward network, which is applied to each position separately and identically.\n    This consists of two linear transformations with a ReLU activation in between.\n    Another way of describing this is as two convolutions with kernel size 1.\n    \"\"\"\n    def __init__(self, d_model: int = 512, d_ff: int = 2048,\n                 dropout_p: float = 0.3, ffnet_style: str = 'ff') -> None:\n        super(PositionWiseFeedForwardNet, self).__init__()\n        self.ffnet_style = ffnet_style.lower()\n        if self.ffnet_style == 'ff':\n            self.feed_forward = nn.Sequential(\n                Linear(d_model, d_ff),\n                nn.Dropout(dropout_p),\n                nn.ReLU(),\n                Linear(d_ff, d_model),\n                nn.Dropout(dropout_p),\n            )\n\n        elif self.ffnet_style == 'conv':\n            self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n            self.relu = nn.ReLU()\n            self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n\n        else:\n            raise ValueError(\"Unsupported mode: {0}\".format(self.mode))\n\n    def forward(self, inputs: Tensor) -> Tensor:\n        if self.ffnet_style == 'conv':\n            output = self.conv1(inputs.transpose(1, 2))\n            output = self.relu(output)\n            return self.conv2(output).transpose(1, 2)\n\n        return self.feed_forward(inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile attention.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom modules import Linear\nfrom torch import Tensor\nfrom typing import Optional, Tuple\n\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n    Compute the dot products of the query with all keys, divide each by sqrt(dim),\n    and apply a softmax function to obtain the weights on the values\n\n    Args: dim, mask\n        dim (int): dimension of attention\n        mask (torch.Tensor): tensor containing indices to be masked\n\n    Inputs: query, key, value, mask\n        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n        - **mask** (-): tensor containing indices to be masked\n\n    Returns: context, attn\n        - **context**: tensor containing the context vector from attention mechanism.\n        - **attn**: tensor containing the attention (alignment) from the encoder outputs.\n    \"\"\"\n    def __init__(self, dim: int) -> None:\n        super(ScaledDotProductAttention, self).__init__()\n        self.sqrt_dim = np.sqrt(dim)\n\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n\n        if mask is not None:\n            score.masked_fill_(mask, -1e9)\n\n        attn = F.softmax(score, -1)\n        context = torch.bmm(attn, value)\n        return context, attn\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-Head Attention proposed in \"Attention Is All You Need\"\n    Instead of performing a single attention function with d_model-dimensional keys, values, and queries,\n    project the queries, keys and values h times with different, learned linear projections to d_head dimensions.\n    These are concatenated and once again projected, resulting in the final values.\n    Multi-head attention allows the model to jointly attend to information from different representation\n    subspaces at different positions.\n\n    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) · W_o\n        where head_i = Attention(Q · W_q, K · W_k, V · W_v)\n\n    Args:\n        d_model (int): The dimension of keys / values / quries (default: 512)\n        num_heads (int): The number of attention heads. (default: 8)\n\n    Inputs: query, key, value, mask\n        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n        - **mask** (-): tensor containing indices to be masked\n\n    Returns: output, attn\n        - **output** (batch, output_len, dimensions): tensor containing the attended output features.\n        - **attn** (batch * num_heads, v_len): tensor containing the attention (alignment) from the encoder outputs.\n    \"\"\"\n    def __init__(self, d_model: int = 512, num_heads: int = 8) -> None:\n        super(MultiHeadAttention, self).__init__()\n\n        assert d_model % num_heads == 0, \"hidden_dim % num_heads should be zero.\"\n\n        self.d_head = int(d_model / num_heads)\n        self.num_heads = num_heads\n        self.query_proj = Linear(d_model, self.d_head * num_heads)\n        self.key_proj = Linear(d_model, self.d_head * num_heads)\n        self.value_proj = Linear(d_model, self.d_head * num_heads)\n        self.sqrt_dim = np.sqrt(d_model)\n        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n        batch_size = value.size(0)\n\n        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)        # BxK_LENxNxD\n        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n\n        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n\n        if mask is not None:\n            mask = mask.repeat(self.num_heads, 1, 1)\n\n        context, attn = self.scaled_dot_attn(query, key, value, mask)\n        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n\n        return context, attn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile decoder.py\n\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch import Tensor\nfrom typing import Optional, Any, Tuple\n\nfrom attention import MultiHeadAttention\nfrom embeddings import Embedding, PositionalEncoding\nfrom mask import get_attn_pad_mask, get_attn_subsequent_mask\nfrom modules import Linear\nfrom sublayers import AddNorm, PositionWiseFeedForwardNet\n\n\nclass SpeechTransformerDecoderLayer(nn.Module):\n    \"\"\"\n    DecoderLayer is made up of self-attention, multi-head attention and feedforward network.\n    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n\n    Args:\n        d_model: dimension of model (default: 512)\n        num_heads: number of attention heads (default: 8)\n        d_ff: dimension of feed forward network (default: 2048)\n        dropout_p: probability of dropout (default: 0.3)\n        ffnet_style: style of feed forward network [ff, conv] (default: ff)\n    \"\"\"\n\n    def __init__(\n            self,\n            d_model: int = 512,             # dimension of model\n            num_heads: int = 8,             # number of attention heads\n            d_ff: int = 2048,               # dimension of feed forward network\n            dropout_p: float = 0.3,         # probability of dropout\n            ffnet_style: str = 'ff'         # style of feed forward network\n    ) -> None:\n        super(SpeechTransformerDecoderLayer, self).__init__()\n        self.self_attention = AddNorm(MultiHeadAttention(d_model, num_heads), d_model)\n        self.memory_attention = AddNorm(MultiHeadAttention(d_model, num_heads), d_model)\n        self.feed_forward = AddNorm(PositionWiseFeedForwardNet(d_model, d_ff, dropout_p, ffnet_style), d_model)\n\n    def forward(\n            self,\n            inputs: Tensor,\n            encoder_outputs: Tensor,\n            self_attn_mask: Optional[Any] = None,\n            memory_mask: Optional[Any] = None\n    ) -> Tuple[Tensor, Tensor, Tensor]:\n        memory = encoder_outputs\n        output, self_attn = self.self_attention(inputs, inputs, inputs, self_attn_mask)\n        output, memory_attn = self.memory_attention(output, memory, memory, memory_mask)\n        output = self.feed_forward(output)\n        return output, self_attn, memory_attn\n\n\nclass SpeechTransformerDecoder(nn.Module):\n    r\"\"\"\n    The TransformerDecoder is composed of a stack of N identical layers.\n    Each layer has three sub-layers. The first is a multi-head self-attention mechanism,\n    and the second is a multi-head attention mechanism, third is a feed-forward network.\n\n    Args:\n        num_classes: umber of classes\n        d_model: dimension of model\n        d_ff: dimension of feed forward network\n        num_layers: number of decoder layers\n        num_heads: number of attention heads\n        ffnet_style: style of feed forward network\n        dropout_p: probability of dropout\n        pad_id: identification of pad token\n        eos_id: identification of end of sentence token\n    \"\"\"\n\n    def __init__(\n            self,\n            num_classes: int,\n            d_model: int = 512,\n            d_ff: int = 2048,\n            num_layers: int = 6,\n            num_heads: int = 8,\n            ffnet_style: str = 'ff',\n            dropout_p: float = 0.3,\n            pad_id: int = 0,\n            sos_id: int = 1,\n            eos_id: int = 2,\n            max_length: int = 128,\n    ) -> None:\n        super(SpeechTransformerDecoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.embedding = Embedding(num_classes, pad_id, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        self.input_dropout = nn.Dropout(p=dropout_p)\n        self.layers = nn.ModuleList([\n            SpeechTransformerDecoderLayer(d_model, num_heads, d_ff, dropout_p, ffnet_style) for _ in range(num_layers)\n        ])\n        self.pad_id = pad_id\n        self.sos_id = sos_id\n        self.eos_id = eos_id\n        self.max_length = max_length\n        self.fc = nn.Sequential(\n            nn.LayerNorm(d_model),\n            Linear(d_model, num_classes, bias=False),\n        )\n\n    def forward_step(\n            self,\n            decoder_inputs,\n            decoder_input_lengths,\n            encoder_outputs,\n            encoder_output_lengths,\n            positional_encoding_length,\n    ) -> Tensor:\n        dec_self_attn_pad_mask = get_attn_pad_mask(\n            decoder_inputs, decoder_input_lengths, decoder_inputs.size(1)\n        )\n        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(decoder_inputs)\n        self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n\n        encoder_attn_mask = get_attn_pad_mask(\n            encoder_outputs, encoder_output_lengths, decoder_inputs.size(1)\n        )\n\n        outputs = self.embedding(decoder_inputs) + self.positional_encoding(positional_encoding_length)\n        outputs = self.input_dropout(outputs)\n\n        for layer in self.layers:\n            outputs, self_attn, memory_attn = layer(\n                inputs=outputs,\n                encoder_outputs=encoder_outputs,\n                self_attn_mask=self_attn_mask,\n                memory_mask=encoder_attn_mask,\n            )\n\n        return outputs\n\n    def forward(\n            self,\n            encoder_outputs: Tensor,\n            targets: Optional[torch.LongTensor] = None,\n            encoder_output_lengths: Tensor = None,\n            target_lengths: Tensor = None,\n            teacher_forcing_ratio: float = 1.0,\n    ) -> Tensor:\n        r\"\"\"\n        Forward propagate a `encoder_outputs` for training.\n\n        Args:\n            targets (torch.LongTensor): A target sequence passed to decoders. `IntTensor` of size\n                ``(batch, seq_length)``\n            encoder_outputs (torch.FloatTensor): A output sequence of encoders. `FloatTensor` of size\n                ``(batch, seq_length, dimension)``\n            encoder_output_lengths (torch.LongTensor): The length of encoders outputs. ``(batch)``\n            teacher_forcing_ratio (float): ratio of teacher forcing\n\n        Returns:\n            * logits (torch.FloatTensor): Log probability of model predictions.\n        \"\"\"\n        batch_size = encoder_outputs.size(0)\n        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        if targets is not None and use_teacher_forcing:\n            targets = targets[targets != self.eos_id].view(batch_size, -1)\n            target_length = targets.size(1)\n\n            outputs = self.forward_step(\n                decoder_inputs=targets,\n                decoder_input_lengths=target_lengths,\n                encoder_outputs=encoder_outputs,\n                encoder_output_lengths=encoder_output_lengths,\n                positional_encoding_length=target_length,\n            )\n            return self.fc(outputs).log_softmax(dim=-1)\n\n        # Inference\n        else:\n            logits = list()\n\n            input_var = encoder_outputs.new_zeros(batch_size, self.max_length).long()\n            input_var = input_var.fill_(self.pad_id)\n            input_var[:, 0] = self.sos_id\n\n            for di in range(1, self.max_length):\n                input_lengths = torch.IntTensor(batch_size).fill_(di)\n\n                outputs = self.forward_step(\n                    decoder_inputs=input_var[:, :di],\n                    decoder_input_lengths=input_lengths,\n                    encoder_outputs=encoder_outputs,\n                    encoder_output_lengths=encoder_output_lengths,\n                    positional_encoding_length=di,\n                )\n                step_output = self.fc(outputs).log_softmax(dim=-1)\n\n                logits.append(step_output[:, -1, :])\n                input_var = logits[-1].topk(1)[1]\n\n            return torch.stack(logits, dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile encoder.py\n\nimport torch.nn as nn\nfrom torch import Tensor\nfrom typing import Tuple, Optional, Any\n\nfrom attention import MultiHeadAttention\nfrom convolution import VGGExtractor\nfrom embeddings import PositionalEncoding\nfrom mask import get_attn_pad_mask\nfrom modules import Linear\nfrom sublayers import AddNorm, PositionWiseFeedForwardNet\n\n\nclass SpeechTransformerEncoderLayer(nn.Module):\n    \"\"\"\n    EncoderLayer is made up of self-attention and feedforward network.\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n\n    Args:\n        d_model: dimension of model (default: 512)\n        num_heads: number of attention heads (default: 8)\n        d_ff: dimension of feed forward network (default: 2048)\n        dropout_p: probability of dropout (default: 0.3)\n        ffnet_style: style of feed forward network [ff, conv] (default: ff)\n    \"\"\"\n\n    def __init__(\n            self,\n            d_model: int = 512,             # dimension of model\n            num_heads: int = 8,             # number of attention heads\n            d_ff: int = 2048,               # dimension of feed forward network\n            dropout_p: float = 0.3,         # probability of dropout\n            ffnet_style: str = 'ff'         # style of feed forward network\n    ) -> None:\n        super(SpeechTransformerEncoderLayer, self).__init__()\n        self.self_attention = AddNorm(MultiHeadAttention(d_model, num_heads), d_model)\n        self.feed_forward = AddNorm(PositionWiseFeedForwardNet(d_model, d_ff, dropout_p, ffnet_style), d_model)\n\n    def forward(self, inputs: Tensor, self_attn_mask: Optional[Any] = None) -> Tuple[Tensor, Tensor]:\n        output, attn = self.self_attention(inputs, inputs, inputs, self_attn_mask)\n        output = self.feed_forward(output)\n        return output, attn\n\n\nclass SpeechTransformerEncoder(nn.Module):\n    \"\"\"\n    The TransformerEncoder is composed of a stack of N identical layers.\n    Each layer has two sub-layers. The first is a multi-head self-attention mechanism,\n    and the second is a simple, position-wise fully connected feed-forward network.\n\n    Args:\n        d_model: dimension of model (default: 512)\n        input_dim: dimension of feature vector (default: 80)\n        d_ff: dimension of feed forward network (default: 2048)\n        num_layers: number of encoder layers (default: 6)\n        num_heads: number of attention heads (default: 8)\n        ffnet_style: style of feed forward network [ff, conv] (default: ff)\n        dropout_p:  probability of dropout (default: 0.3)\n        pad_id: identification of pad token (default: 0)\n\n    Inputs:\n        - **inputs**: list of sequences, whose length is the batch size and within which each sequence is list of tokens\n        - **input_lengths**: list of sequence lengths\n    \"\"\"\n\n    def __init__(\n            self,\n            d_model: int = 512,\n            input_dim: int = 80,\n            d_ff: int = 2048,\n            num_layers: int = 6,\n            num_heads: int = 8,\n            ffnet_style: str = 'ff',\n            dropout_p: float = 0.3,\n            pad_id: int = 0,\n    ) -> None:\n        super(SpeechTransformerEncoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.pad_id = pad_id\n        self.conv = VGGExtractor(input_dim)\n        self.input_proj = Linear(self.conv.get_output_dim(), d_model)\n        self.input_dropout = nn.Dropout(p=dropout_p)\n        self.positional_encoding = PositionalEncoding(d_model)\n        self.layers = nn.ModuleList(\n            [SpeechTransformerEncoderLayer(d_model, num_heads, d_ff, dropout_p, ffnet_style) for _ in range(num_layers)]\n        )\n\n    def forward(self, inputs: Tensor, input_lengths: Tensor = None) -> Tuple[Tensor, Tensor]:\n        conv_outputs, output_lengths = self.conv(inputs, input_lengths)\n\n        self_attn_mask = get_attn_pad_mask(conv_outputs, output_lengths, conv_outputs.size(1))\n        print('conv_outputs: ', conv_outputs.size())\n        outputs = self.input_proj(conv_outputs)\n        outputs += self.positional_encoding(outputs.size(1))\n        outputs = self.input_dropout(outputs)\n\n        for layer in self.layers:\n            outputs, attn = layer(outputs, self_attn_mask)\n\n        return outputs, output_lengths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile beam_decoder.py\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom decoder import SpeechTransformerDecoder\n\n\nclass BeamTransformerDecoder(nn.Module):\n    def __init__(self, decoder: SpeechTransformerDecoder, batch_size: int, beam_size: int = 3) -> None:\n        super(BeamTransformerDecoder, self).__init__()\n        self.decoder = decoder\n        self.beam_size = beam_size\n        self.sos_id = decoder.sos_id\n        self.pad_id = decoder.pad_id\n        self.eos_id = decoder.eos_id\n        self.ongoing_beams = None\n        self.cumulative_ps = None\n        self.finished = [[] for _ in range(batch_size)]\n        self.finished_ps = [[] for _ in range(batch_size)]\n        self.forward_step = decoder.forward_step\n        self.use_cuda = True if torch.cuda.is_available() else False\n\n    def _inflate(self, tensor: Tensor, n_repeat: int, dim: int) -> Tensor:\n        repeat_dims = [1] * len(tensor.size())\n        repeat_dims[dim] *= n_repeat\n\n        return tensor.repeat(*repeat_dims)\n\n    def _get_successor(\n            self,\n            current_ps: Tensor,\n            current_vs: Tensor,\n            finished_ids: tuple,\n            num_successor: int,\n            eos_count: int,\n            k: int\n    ) -> int:\n        finished_batch_idx, finished_idx = finished_ids\n\n        successor_ids = current_ps.topk(k + num_successor)[1]\n        successor_idx = successor_ids[finished_batch_idx, -1]\n\n        successor_p = current_ps[finished_batch_idx, successor_idx]\n        successor_v = current_vs[finished_batch_idx, successor_idx]\n\n        prev_status_idx = (successor_idx // k)\n        prev_status = self.ongoing_beams[finished_batch_idx, prev_status_idx]\n        prev_status = prev_status.view(-1)[:-1]\n\n        successor = torch.cat([prev_status, successor_v.view(1)])\n\n        if int(successor_v) == self.eos_id:\n            self.finished[finished_batch_idx].append(successor)\n            self.finished_ps[finished_batch_idx].append(successor_p)\n            eos_count = self._get_successor(\n                current_ps=current_ps,\n                current_vs=current_vs,\n                finished_ids=finished_ids,\n                num_successor=num_successor + eos_count,\n                eos_count=eos_count + 1,\n                k=k,\n            )\n\n        else:\n            self.ongoing_beams[finished_batch_idx, finished_idx] = successor\n            self.cumulative_ps[finished_batch_idx, finished_idx] = successor_p\n\n        return eos_count\n\n    def _get_hypothesis(self):\n        predictions = list()\n\n        for batch_idx, batch in enumerate(self.finished):\n            # if there is no terminated sentences, bring ongoing sentence which has the highest probability instead\n            if len(batch) == 0:\n                prob_batch = self.cumulative_ps[batch_idx]\n                top_beam_idx = int(prob_batch.topk(1)[1])\n                predictions.append(self.ongoing_beams[batch_idx, top_beam_idx])\n\n            # bring highest probability sentence\n            else:\n                top_beam_idx = int(torch.FloatTensor(self.finished_ps[batch_idx]).topk(1)[1])\n                predictions.append(self.finished[batch_idx][top_beam_idx])\n\n        predictions = self._fill_sequence(predictions)\n        return predictions\n\n    def _is_all_finished(self, k: int) -> bool:\n        for done in self.finished:\n            if len(done) < k:\n                return False\n\n        return True\n\n    def _fill_sequence(self, y_hats: list) -> Tensor:\n        batch_size = len(y_hats)\n        max_length = -1\n\n        for y_hat in y_hats:\n            if len(y_hat) > max_length:\n                max_length = len(y_hat)\n\n        matched = torch.zeros((batch_size, max_length), dtype=torch.long)\n\n        for batch_idx, y_hat in enumerate(y_hats):\n            matched[batch_idx, :len(y_hat)] = y_hat\n            matched[batch_idx, len(y_hat):] = int(self.pad_id)\n\n        return matched\n\n    def forward(self, encoder_outputs: torch.FloatTensor, encoder_output_lengths: torch.FloatTensor):\n        batch_size = encoder_outputs.size(0)\n\n        decoder_inputs = torch.IntTensor(batch_size, self.decoder.max_length).fill_(self.sos_id).long()\n        decoder_input_lengths = torch.IntTensor(batch_size).fill_(1)\n\n        outputs = self.forward_step(\n            decoder_inputs=decoder_inputs[:, :1],\n            decoder_input_lengths=decoder_input_lengths,\n            encoder_outputs=encoder_outputs,\n            encoder_output_lengths=encoder_output_lengths,\n            positional_encoding_length=1,\n        )\n        step_outputs = self.decoder.fc(outputs).log_softmax(dim=-1)\n        self.cumulative_ps, self.ongoing_beams = step_outputs.topk(self.beam_size)\n\n        self.ongoing_beams = self.ongoing_beams.view(batch_size * self.beam_size, 1)\n        self.cumulative_ps = self.cumulative_ps.view(batch_size * self.beam_size, 1)\n\n        decoder_inputs = torch.IntTensor(batch_size * self.beam_size, 1).fill_(self.sos_id)\n        decoder_inputs = torch.cat((decoder_inputs, self.ongoing_beams), dim=-1)  # bsz * beam x 2\n\n        encoder_dim = encoder_outputs.size(2)\n        encoder_outputs = self._inflate(encoder_outputs, self.beam_size, dim=0)\n        encoder_outputs = encoder_outputs.view(self.beam_size, batch_size, -1, encoder_dim)\n        encoder_outputs = encoder_outputs.transpose(0, 1)\n        encoder_outputs = encoder_outputs.reshape(batch_size * self.beam_size, -1, encoder_dim)\n\n        encoder_output_lengths = encoder_output_lengths.unsqueeze(1).repeat(1, self.beam_size).view(-1)\n\n        for di in range(2, self.decoder.max_length):\n            if self._is_all_finished(self.beam_size):\n                break\n\n            decoder_input_lengths = torch.LongTensor(batch_size * self.beam_size).fill_(di)\n\n            step_outputs = self.forward_step(\n                decoder_inputs=decoder_inputs[:, :di],\n                decoder_input_lengths=decoder_input_lengths,\n                encoder_outputs=encoder_outputs,\n                encoder_output_lengths=encoder_output_lengths,\n                positional_encoding_length=di,\n            )\n            step_outputs = self.decoder.fc(step_outputs).log_softmax(dim=-1)\n\n            step_outputs = step_outputs.view(batch_size, self.beam_size, -1, 10)\n            current_ps, current_vs = step_outputs.topk(self.beam_size)\n\n            # TODO: Check transformer's beam search\n            current_ps = current_ps[:, :, -1, :]\n            current_vs = current_vs[:, :, -1, :]\n\n            self.cumulative_ps = self.cumulative_ps.view(batch_size, self.beam_size)\n            self.ongoing_beams = self.ongoing_beams.view(batch_size, self.beam_size, -1)\n\n            current_ps = (current_ps.permute(0, 2, 1) + self.cumulative_ps.unsqueeze(1)).permute(0, 2, 1)\n            current_ps = current_ps.view(batch_size, self.beam_size ** 2)\n            current_vs = current_vs.contiguous().view(batch_size, self.beam_size ** 2)\n\n            self.cumulative_ps = self.cumulative_ps.view(batch_size, self.beam_size)\n            self.ongoing_beams = self.ongoing_beams.view(batch_size, self.beam_size, -1)\n\n            topk_current_ps, topk_status_ids = current_ps.topk(self.beam_size)\n            prev_status_ids = (topk_status_ids // self.beam_size)\n\n            topk_current_vs = torch.zeros((batch_size, self.beam_size), dtype=torch.long)\n            prev_status = torch.zeros(self.ongoing_beams.size(), dtype=torch.long)\n\n            for batch_idx, batch in enumerate(topk_status_ids):\n                for idx, topk_status_idx in enumerate(batch):\n                    topk_current_vs[batch_idx, idx] = current_vs[batch_idx, topk_status_idx]\n                    prev_status[batch_idx, idx] = self.ongoing_beams[batch_idx, prev_status_ids[batch_idx, idx]]\n\n            self.ongoing_beams = torch.cat([prev_status, topk_current_vs.unsqueeze(2)], dim=2)\n            self.cumulative_ps = topk_current_ps\n\n            if torch.any(topk_current_vs == self.eos_id):\n                finished_ids = torch.where(topk_current_vs == self.eos_id)\n                num_successors = [1] * batch_size\n\n                for (batch_idx, idx) in zip(*finished_ids):\n                    self.finished[batch_idx].append(self.ongoing_beams[batch_idx, idx])\n                    self.finished_ps[batch_idx].append(self.cumulative_ps[batch_idx, idx])\n\n                    if self.beam_size != 1:\n                        eos_count = self._get_successor(\n                            current_ps=current_ps,\n                            current_vs=current_vs,\n                            finished_ids=(batch_idx, idx),\n                            num_successor=num_successors[batch_idx],\n                            eos_count=1,\n                            k=self.beam_size,\n                        )\n                        num_successors[batch_idx] += eos_count\n\n            ongoing_beams = self.ongoing_beams.clone().view(batch_size * self.beam_size, -1)\n            decoder_inputs = torch.cat((decoder_inputs, ongoing_beams[:, :-1]), dim=-1)\n\n        return self._get_hypothesis()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model.py\n\nimport torch.nn as nn\nfrom torch import Tensor\nfrom typing import Optional, Union\n\nfrom beam_decoder import BeamTransformerDecoder\nfrom decoder import SpeechTransformerDecoder\nfrom encoder import SpeechTransformerEncoder\n\n\nclass SpeechTransformer(nn.Module):\n    \"\"\"\n    A Speech Transformer model. User is able to modify the attributes as needed.\n    The model is based on the paper \"Attention Is All You Need\".\n\n    Args:\n        num_classes (int): the number of classfication\n        d_model (int): dimension of model (default: 512)\n        input_dim (int): dimension of input\n        pad_id (int): identification of <PAD_token>\n        eos_id (int): identification of <EOS_token>\n        d_ff (int): dimension of feed forward network (default: 2048)\n        num_encoder_layers (int): number of encoder layers (default: 6)\n        num_decoder_layers (int): number of decoder layers (default: 6)\n        num_heads (int): number of attention heads (default: 8)\n        dropout_p (float): dropout probability (default: 0.3)\n        ffnet_style (str): if poswise_ffnet is 'ff', position-wise feed forware network to be a feed forward,\n            otherwise, position-wise feed forward network to be a convolution layer. (default: ff)\n\n    Inputs: inputs, input_lengths, targets, teacher_forcing_ratio\n        - **inputs** (torch.Tensor): tensor of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs. This information is forwarded to the encoder.\n        - **input_lengths** (torch.Tensor): tensor of sequences, whose contains length of inputs.\n        - **targets** (torch.Tensor): tensor of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs. This information is forwarded to the decoder.\n\n    Returns: output\n        - **output**: tensor containing the outputs\n    \"\"\"\n\n    def __init__(\n            self,\n            num_classes: int,\n            d_model: int = 512,\n            input_dim: int = 80,\n            pad_id: int = 0,\n            sos_id: int = 1,\n            eos_id: int = 2,\n            d_ff: int = 2048,\n            num_heads: int = 8,\n            num_encoder_layers: int = 6,\n            num_decoder_layers: int = 6,\n            dropout_p: float = 0.3,\n            ffnet_style: str = 'ff',\n            extractor: str = 'vgg',\n            joint_ctc_attention: bool = False,\n            max_length: int = 128,\n    ) -> None:\n        super(SpeechTransformer, self).__init__()\n\n        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n\n        self.num_classes = num_classes\n        self.extractor = extractor\n        self.joint_ctc_attention = joint_ctc_attention\n        self.sos_id = sos_id\n        self.eos_id = eos_id\n        self.pad_id = pad_id\n        self.max_length = max_length\n\n        self.encoder = SpeechTransformerEncoder(\n            d_model=d_model,\n            input_dim=input_dim,\n            d_ff=d_ff,\n            num_layers=num_encoder_layers,\n            num_heads=num_heads,\n            ffnet_style=ffnet_style,\n            dropout_p=dropout_p,\n            pad_id=pad_id,\n        )\n\n        self.decoder = SpeechTransformerDecoder(\n            num_classes=num_classes,\n            d_model=d_model,\n            d_ff=d_ff,\n            num_layers=num_decoder_layers,\n            num_heads=num_heads,\n            ffnet_style=ffnet_style,\n            dropout_p=dropout_p,\n            pad_id=pad_id,\n            sos_id=sos_id,\n            eos_id=eos_id,\n            max_length = max_length,\n        )\n\n    def set_beam_decoder(self, batch_size: int = None, beam_size: int = 3):\n        \"\"\" Setting beam search decoder \"\"\"\n        self.decoder = BeamTransformerDecoder(\n            decoder=self.decoder,\n            batch_size=batch_size,\n            beam_size=beam_size,\n        )\n\n    def forward(\n            self,\n            inputs: Tensor,\n            input_lengths: Tensor,\n            targets: Optional[Tensor] = None,\n            target_lengths: Optional[Tensor] = None,\n    ) -> Union[Tensor, tuple]:\n        \"\"\"\n        inputs (torch.FloatTensor): (batch_size, sequence_length, dimension)\n        input_lengths (torch.LongTensor): (batch_size)\n        \"\"\"\n        logits = None\n        #print('inputs: ', inputs)\n        #print('inputs lengths model: ', inputs.size())\n        encoder_outputs, encoder_output_lengths = self.encoder(inputs, input_lengths)\n        if isinstance(self.decoder, BeamTransformerDecoder):\n            predictions = self.decoder(encoder_outputs, encoder_output_lengths)\n        else:\n            logits = self.decoder(\n                encoder_outputs=encoder_outputs,\n                encoder_output_lengths=encoder_output_lengths,\n                targets=targets,\n                teacher_forcing_ratio=0.0,\n                target_lengths=target_lengths,\n            )\n            predictions = logits.max(-1)[1]\n\n        return predictions, logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport logging\nimport subprocess\nfrom ctcdecode import CTCBeamDecoder\nimport jiwer\nimport random\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom read_emg import EMGDataset, SizeAwareSampler\nfrom data_utils import combine_fixed_length, decollate_tensor\nfrom model import SpeechTransformer\n\n#from torch.utils.checkpoint import checkpoint\n\nfrom absl import flags\nFLAGS = flags.FLAGS\n\ngc.get_threshold()\ngc.get_count()\ngc.collect()\ngc.get_count()\n\ntorch.cuda.empty_cache()\ntorch.cuda.set_per_process_memory_fraction(0.6, device=0)\ntorch.cuda.max_split_size_mb(512, device=0)\n\n\ndef test(model, testset, device):  #avalia o desempenho de um modelo em um conjunto de teste\n    model.eval() #Define o modelo no modo de avaliação, desativando gradientes e camadas como o Dropout.\n\n    blank_id = len(testset.text_transform.chars)\n    decoder = CTCBeamDecoder(testset.text_transform.chars+'_', blank_id=blank_id, log_probs_input=True,  #CTCBeamDecoder é usado para realizar a decodificação do resultado do modelo em sequências de texto.\n            model_path='/kaggle/input/librispeech-4gram-language-model/4-gram-librispeech.bin', alpha=1.5, beta=1.85)\n\n    dataloader = torch.utils.data.DataLoader(testset, batch_size=1)\n    references = []\n    predictions = []\n    with torch.no_grad():\n        for example in dataloader:\n            X = example['emg'].to(device)\n            X_raw = example['raw_emg'].to(device)\n            sess = example['session_ids'].to(device)\n            \n            xraw_lenght= X_raw.size(1)\n            pred  = F.log_softmax(model(X_raw, xraw_lenght), -1)  #Aplica a função de softmax aos logits de saída do modelo e, em seguida, calcula o logaritmo das probabilidades resultantes.\n\n            beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred) #Usa o decodificador CTC para obter as sequências mais prováveis ​​a partir dos logits previstos pelo modelo.\n            pred_int = beam_results[0,0,:out_lens[0,0]].tolist()\n\n            pred_text = testset.text_transform.int_to_text(pred_int)  #Converte as previsões em formato de lista de inteiros em texto usando a função int_to_text da transformação de texto do conjunto de teste.\n            #print('pred_text: ', pred_text)\n            target_text = testset.text_transform.clean_text(example['text'][0])  #Obtém o texto verdadeiro correspondente ao exemplo atual do conjunto de teste.\n            #print('target text: ',target_text)\n            \n            if len(target_text) != 0:\n                references.append(target_text)\n                #print('references: ', references)\n                predictions.append(pred_text)\n                #print('predictions: ', predictions)\n                #print(len(references))\n            else:\n                continue\n            # ^Adiciona as referências e as previsões às listas correspondentes.\n    print('references: ', references)\n    print('predictions: ', predictions)\n    model.train() #Define o modelo de volta no modo de treinamento.\n    return jiwer.wer(references, predictions) #Calcula a taxa de erro de palavra (WER) usando a biblioteca jiwer, comparando as referências e as previsões.\n\n\n\ndef train_model(trainset, devset, device, n_epochs=50):  #executa o ciclo de treinamento do modelo, ajustando gradualmente os parâmetros do modelo com base nas previsões e calculando a perda. \n                #O agendamento da taxa de aprendizado ajuda a controlar a taxa de convergência durante o treinamento. ORIGINAL 200\n    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000)) #ORIGINAL 128000\n        #^ configura o DataLoader para fornecer os dados de treinamento em lotes, com opções adicionais de otimização para dispositivos CUDA.\n\n    n_chars = len(devset.text_transform.chars)  # Calcula o número de caracteres únicos no conjunto de dados de validação (devset). O atributo text_transform.chars representa a lista de caracteres nos dados de texto. \n    model = SpeechTransformer(num_classes=n_chars+1, d_model=144, num_heads=4, input_dim=devset.num_features).to(device)\n   #devset.num_features representa o número de recursos (ou características) de entrada do modelo, enquanto n_chars+1 representa o número de classes de saída\n                    #do modelo. Nesse caso, o número de classes de saída (opções de fonemas) é incrementado em 1 porque é adicionado um símbolo de espaço em branco (blank symbol) adicional. \n\n    if FLAGS.start_training_from is not None:\n        state_dict = torch.load(FLAGS.start_training_from)\n        model.load_state_dict(state_dict, strict=False)\n        #^se houver um modelo pré-treinado fornecido para continuar o treinamento, o estado do modelo é carregado a partir do arquivo especificado por FLAGS.start_training_from usando a função torch.load(). \n        # Em seguida, o estado do modelo é carregado no modelo atual. O argumento strict=False permite que os pesos sejam carregados mesmo que a estrutura do modelo não seja idêntica, o que é útil para carregar \n        # partes de um modelo pré-treinado em um modelo com arquitetura ligeiramente modificada.\n\n    optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2) #O otimizador é configurado usando o algoritmo AdamW. Ele otimiza os parâmetros do modelo durante o treinamento. \n    #Os parâmetros model.parameters() fornecem a lista de parâmetros do modelo que serão otimizados. O argumento lr=FLAGS.learning_rate define a taxa de aprendizado inicial para o otimizador. O argumento \n    # weight_decay=FLAGS.l2 configura o termo de decaimento de peso (weight decay), que controla a regularização L2 aplicada aos parâmetros do modelo durante o treinamento.\n    lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5) #O agendador de taxa de aprendizado é configurado usando o torch.optim.lr_scheduler.MultiStepLR. Ele ajusta a taxa\n    #de aprendizado ao longo do treinamento, reduzindo-a em certos marcos (milestones) predefinidos. Os marcos são definidos por milestones=[125, 150, 175], o que significa que a taxa de aprendizado será reduzida \n    # pela metade nessas épocas específicas. O argumento gamma=.5 define o fator de redução da taxa de aprendizado. Portanto, a cada marco especificado, a taxa de aprendizado é multiplicada por gamma, reduzindo-a \n    # pela metade. Isso é útil para controlar a taxa de aprendizado ao longo do treinamento, permitindo ajustes finos à medida que o treinamento progride.\n\n    def set_lr(new_lr):#atualiza a taxa de aprendizado do otimizador. Ela itera sobre os grupos de parâmetros do otimizador (optim.param_groups) e define o valor da taxa de aprendizado ('lr') para new_lr. \n        #Essa função é usada para definir manualmente a taxa de aprendizado em momentos específicos durante o treinamento.\n        for param_group in optim.param_groups:\n            param_group['lr'] = new_lr\n\n    target_lr = FLAGS.learning_rate \n    def schedule_lr(iteration):  # realiza um aquecimento linear da taxa de aprendizado, aumentando-a gradualmente até atingir o valor desejado durante as iterações de aquecimento. Após o término do aquecimento \n        #linear, a taxa de aprendizado será controlada pelo agendador de taxa de aprendizado definido anteriormente.\n        iteration = iteration + 1\n        if iteration <= FLAGS.learning_rate_warmup:\n            set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)\n            \n\n    batch_idx = 0\n    optim.zero_grad()\n    for epoch_idx in range(n_epochs):\n        losses = []\n        total_loss = 0.0\n        for example in dataloader:\n            schedule_lr(batch_idx) #Para cada lote de exemplos no dataloader, o agendamento da taxa de aprendizado é chamado usando o índice do lote atual (schedule_lr(batch_idx)). Isso ajusta a taxa de aprendizado \n            #com base na programação definida anteriormente.\n\n            X = combine_fixed_length(example['emg'], 200).to(device)\n            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)\n            sess = combine_fixed_length(example['session_ids'], 200).to(device)\n            #^ Os dados de entrada são pré-processados para serem alimentados no modelo. Isso inclui a combinação dos comprimentos fixos dos sinais de entrada (X, X_raw e sess) usando a função combine_fixed_length,\n            #  e a padronização das sequências para que tenham o mesmo comprimento usando a função nn.utils.rnn.pad_sequence.\n            xraw_lenght= X_raw.size(1)\n            pred = model(X_raw, xraw_lenght)\n            #print('pred:', pred)\n            #pred, logits = model(X, input_lengths, targets, target_lengths)\n            pred = F.log_softmax(pred, 2) #As previsões são passadas pela função de ativação softmax (F.log_softmax) para obter uma distribuição de probabilidade em cada posição da sequência\n            \n            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc  \n            #^ as previsões são padronizadas usando a função nn.utils.rnn.pad_sequence para ter o formato adequado necessário para calcular a perda CTC.\n            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device) \n            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars) #A perda CTC é calculada comparando as previsões padronizadas (pred) com os rótulos de texto padronizados (y). A função \n            #F.ctc_loss é usada para calcular a perda, levando em consideração os comprimentos das sequências e o símbolo em branco (blank).\n            print(loss)\n            loss = loss / accumulation_steps\n            losses.append(loss.item())\n            loss.requires_grad_()  \n            loss.backward() #Os gradientes da perda são calculados usando a função loss.backward()\n            if (batch_idx+1) % accumulation_steps == 0: #A otimização dos parâmetros ocorre a cada 2 lotes (if (batch_idx+1) % 2 == 0), onde a função optim.step() atualiza os parâmetros do modelo com base nos gradientes acumulados, \n                #e optim.zero_grad() zera os gradientes para o próximo lote.\n                optim.step()\n                optim.zero_grad()\n            batch_idx += 1\n            gc.get_count()\n            gc.collect()\n            gc.get_count()\n            \n        train_loss = np.mean(losses) #A perda média é calculada como a média das perdas em cada lote durante a época atual.\n        val = test(model, devset, device) #O modelo treinado é avaliado no conjunto de validação usando a função test, que retorna a taxa de erro de palavra (WER) calculada com base nas previsões do modelo.\n        lr_sched.step() #O agendador de taxa de aprendizado (lr_sched) é atualizado usando o método lr_sched.step(), que ajusta a taxa de aprendizado com base no número atual de épocas.\n        logging.info(f'finished epoch {epoch_idx+1}/{n_epochs} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')\n        print(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')\n        torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt')) \n        #Salva os parâmetros do modelo no disco.\n        #torch.cuda.empty_cache()\n\n    model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters  #Recarrega os melhores parâmetros do modelo com base no arquivo salvo.\n    return model\n\ndef evaluate_saved(): #avalia um modelo previamente treinado em um conjunto de teste\n    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n    testset = EMGDataset(test=True) #Cria uma instância do conjunto de dados de teste (testset) usando a classe EMGDataset, indicando que é para o conjunto de teste, por meio do argumento test=True.\n    n_chars = len(testset.text_transform.chars) #Obtém o número de classes de saída do modelo, que é igual ao comprimento do alfabeto usado na transformação de texto (text_transform) do conjunto de teste.\n    model = Model(testset.num_features, n_chars+1).to(device) #Cria uma instância do modelo (model) usando a classe Model, especificando o número de recursos de entrada (testset.num_features) e o número de classes \n    #de saída (n_chars+1). O modelo é movido para o dispositivo especificado.\n    model.load_state_dict(torch.load(FLAGS.evaluate_saved)) #Carrega os parâmetros salvos do modelo a partir do arquivo indicado pela flag\n    print('WER:', test(model, testset, device)) #Realiza a avaliação do modelo carregado no conjunto de teste usando a função test(), e imprime a taxa de erro de palavra (WER) resultante.\n\ndef main():\n    os.makedirs(FLAGS.output_directory, exist_ok=True) # Cria o diretório de saída especificado pela sinalização FLAGS.output_directory se ele não existir. A sinalização exist_ok=True permite que o diretório \n    #seja criado mesmo se ele já existir.\n    logging.basicConfig(handlers=[ #Configura o sistema de registro (logging) para gravar mensagens em um arquivo de log e exibi-las no console.\n            logging.FileHandler(os.path.join(FLAGS.output_directory, 'log.txt'), 'w'),\n            logging.StreamHandler()\n            ], level=logging.INFO, format=\"%(message)s\")\n\n    logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)\n    logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)\n\n    logging.info(sys.argv)\n\n    dataset = EMGDataset(dev=False,test=False) #Cria uma instância do conjunto de dados de treinamento (trainset) usando a classe EMGDataset, indicando que não é para usar o conjunto de desenvolvimento \n    #(dev=False) e o conjunto de teste (test=False).\n\n    fraction = 0.1  # Fração dos exemplos desejados\n    #trainset = dataset\n    trainset = dataset.subset(fraction)\n\n    devset = EMGDataset(dev=True) # Cria uma instância do conjunto de dados de desenvolvimento (devset) usando a classe EMGDataset, indicando que é para usar o conjunto de desenvolvimento (dev=True).\n    logging.info('output example: %s', devset.example_indices[0])\n    logging.info('train / dev split: %d %d',len(trainset),len(devset))\n\n    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n\n    model = train_model(trainset, devset, device)\n\nif __name__ == '__main__':\n    if FLAGS.evaluate_saved is not None:\n        evaluate_saved()\n    else:\n        main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}